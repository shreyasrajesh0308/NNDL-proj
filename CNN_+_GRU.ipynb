{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN + GRU.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasrajesh0308/NNDL-proj/blob/main/CNN_%2B_GRU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Paper: "
      ],
      "metadata": {
        "id": "Wy-ORg0nyKtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data \n",
        "\n",
        "Load preprocessed data"
      ],
      "metadata": {
        "id": "5Fl4uJtrAqU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Scu17GL9A7i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "838df8b8-876c-4335-ae38-13398d5b56cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "2gXQ7pXTIqqT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.load(\"/content/drive/MyDrive/project_data/X_test.npy\")\n",
        "y_test = np.load(\"/content/drive/MyDrive/project_data/y_test.npy\")\n",
        "person_train_valid = np.load(\"/content/drive/MyDrive/project_data/person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"/content/drive/MyDrive/project_data/X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"/content/drive/MyDrive/project_data/y_train_valid.npy\")\n",
        "person_test = np.load(\"/content/drive/MyDrive/project_data/person_test.npy\")"
      ],
      "metadata": {
        "id": "foTd2q3JIreb"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n"
      ],
      "metadata": {
        "id": "g6VAQSVEUtIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b23f44a-2fd8-4191-f943-65bd5452715e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train_valid))\n",
        "print(np.unique(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhxR60r5IzuR",
        "outputId": "b34befc1-1871-4ff8-fdf2-db4912372786"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[769 770 771 772]\n",
            "[769 770 771 772]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "y_train_valid = y_train_valid-769\n",
        "y_test = y_test-769"
      ],
      "metadata": {
        "id": "U4K1wzmfI4Rb"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data into train and Valid"
      ],
      "metadata": {
        "id": "L4tuh59BQ-FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6xo7RhdpRBPx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Training data shape: {}'.format(X_train.shape))\n",
        "print ('Valid data shape: {}'.format(X_valid.shape))\n",
        "print ('Training target shape: {}'.format(y_train.shape))\n",
        "print ('Valid target shape: {}'.format(y_valid.shape))"
      ],
      "metadata": {
        "id": "McUbRkeARWXE",
        "outputId": "39d9d9ab-5d12-49ae-8246-ca9e0649c118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1692, 22, 1000)\n",
            "Valid data shape: (423, 22, 1000)\n",
            "Training target shape: (1692,)\n",
            "Valid target shape: (423,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(X,y,sub_sample,average,noise):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    \n",
        "    # Averaging + noise \n",
        "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
        "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
        "    \n",
        "    total_X = np.vstack((total_X, X_average))\n",
        "    total_y = np.hstack((total_y, y))\n",
        "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
        "    \n",
        "    # Subsampling\n",
        "    \n",
        "    for i in range(sub_sample):\n",
        "        \n",
        "        X_subsample = X[:, :, i::sub_sample] + \\\n",
        "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
        "            \n",
        "        total_X = np.vstack((total_X, X_subsample))\n",
        "        total_y = np.hstack((total_y, y))\n",
        "        \n",
        "    \n",
        "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_train_prep,y_train_prep= data_prep(X_train,y_train,2,2,True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ubjiWDpR_W",
        "outputId": "a6982030-c37c-4ff7-bb68-97138ea861e3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (1692, 22, 500)\n",
            "Shape of X after maxpooling: (1692, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (3384, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (6768, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep_test(X,y, sub_sample=2):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_test,y_test = data_prep_test(X_test, y_test,2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhtalewhtI6R",
        "outputId": "35b01219-d616-427c-85be-2f671553d0ef"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (443, 22, 500)\n",
            "Shape of X after maxpooling: (443, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_prep,y_valid_prep = data_prep_test(X_valid, y_valid,2)\n"
      ],
      "metadata": {
        "id": "tu4DoAKFRsip",
        "outputId": "1b111c8b-c219-4fa1-835c-8a41a0fdfa98",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (423, 22, 500)\n",
            "Shape of X after maxpooling: (423, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_prep(X):\n",
        "  # new_X = []\n",
        "  # X = X.tolist()\n",
        "  X = np.swapaxes(X, 0, 1)\n",
        "\n",
        "  sh = (X.shape[1],X.shape[2])\n",
        "  new_X = np.array([[np.zeros(sh), np.zeros(sh), np.zeros(sh), X[0,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), X[1,:,:], X[2,:,:], X[3,:,:], X[4,:,:], X[5,:,:], np.zeros(sh)],\n",
        "                [X[6,:,:],X[7,:,:],X[8,:,:],X[9,:,:],X[10,:,:],X[11,:,:],X[12,:,:]],\n",
        "                [np.zeros(sh), X[13,:,:],X[14,:,:],X[15,:,:],X[16,:,:],X[17,:,:], np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), X[18,:,:],X[19,:,:],X[20,:,:], np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), np.zeros(sh), X[21,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)]])\n",
        "\n",
        "  # for x_seq in X:\n",
        "  #   n = len(X)\n",
        "  #   # new_seq_X = np.array([np.array([np.zeros(n), np.zeros(n), np.zeros(n), np.array(x_seq[0]), np.zeros(n), np.zeros(n), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.array(x_seq[1]), np.array(x_seq[2]), np.array(x_seq[3]), np.array(x_seq[4]), np.array(x_seq[5]), np.zeros(n)]),\n",
        "  #   #              np.array([np.array(x_seq[6]), np.array(x_seq[7]), np.array(x_seq[8]), np.array(x_seq[9]), np.array(x_seq[10]), np.array(x_seq[11]), np.array(x_seq[12])]),\n",
        "  #   #              np.array([np.zeros(n), np.array(x_seq[13]), np.array(x_seq[14]), np.array(x_seq[15]), np.array(x_seq[16]), np.array(x_seq[17]), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.zeros(n), np.array(x_seq[18]), np.array(x_seq[19]), np.array(x_seq[20]), np.zeros(n), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.zeros(n), np.zeros(n), np.array(x_seq[21]), np.zeros(n), np.zeros(n), np.zeros(n)])])\n",
        "  #   # new_X.append(new_seq_X)\n",
        "  #   new_seq_X = np.array([[[0]*n, [0]*n, [0]*n, x_seq[0], [0]*n, [0]*n, [0]*n ],\n",
        "  #                [[0]*n, x_seq[1], x_seq[2], x_seq[3], x_seq[4], x_seq[5], [0]*n ],\n",
        "  #                [x_seq[6], x_seq[7], x_seq[8], x_seq[9], x_seq[10], x_seq[11], x_seq[12]],\n",
        "  #                [[0]*n, x_seq[13], x_seq[14], x_seq[15], x_seq[16], x_seq[17], [0]*n],\n",
        "  #                [[0]*n, [0]*n, x_seq[18], x_seq[19], x_seq[20], [0]*n, [0]*n],\n",
        "  #                [[0]*n, [0]*n, [0]*n, x_seq[21], [0]*n, [0]*n, [0]*n]])\n",
        "  #   new_X.append(new_seq_X)\n",
        "\n",
        "  # new_X = np.array(new_X)\n",
        "  new_X = np.swapaxes(new_X, 0,2)\n",
        "  new_X = np.swapaxes(new_X, 1,2)\n",
        "  return new_X"
      ],
      "metadata": {
        "id": "-u8tro1jwuon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep = spatial_prep(X_train_valid_prep)"
      ],
      "metadata": {
        "id": "flRuB5gPygqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_test = spatial_prep(X_test)"
      ],
      "metadata": {
        "id": "spWmKQWT3qd1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_spatial_prep.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3rD3lYUyumZ",
        "outputId": "dae237fb-2037-4426-fb5f-7634f93481e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 6, 7, 250) (443, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train_prep = keras.utils.to_categorical(y_train_prep, num_classes)\n",
        "y_valid_prep = keras.utils.to_categorical(y_valid_prep, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "NZWq7ZCFq5z9"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_valid_prep.shape,y_train_valid_prep.shape, X_spatial_prep.shape, X_spatial_test.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZfw6CuqeBs",
        "outputId": "88e49b8e-869a-4005-e7f7-8a5721d428bf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 22, 250) (8460, 4) (8460, 6, 7, 250) (443, 6, 7, 250)\n",
            "(443, 22, 250) (443, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep  = np.swapaxes(X_spatial_prep , 1,3)\n",
        "X_spatial_prep = np.swapaxes(X_spatial_prep , 2,3)\n",
        "X_spatial_test  = np.swapaxes(X_spatial_test , 1,3)\n",
        "X_spatial_test = np.swapaxes(X_spatial_test , 2,3)"
      ],
      "metadata": {
        "id": "b3_pN5eAY1bg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep = X_spatial_prep.reshape(*X_spatial_prep.shape, 1)\n",
        "X_spatial_test = X_spatial_test.reshape(*X_spatial_test.shape, 1)"
      ],
      "metadata": {
        "id": "FeiaaX6qX3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_valid_prep.shape,y_train_valid_prep.shape, X_spatial_prep.shape, X_spatial_test.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MC6IQn7YEF-",
        "outputId": "a027cdc4-3580-4762-9cb6-d98ea975314f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 22, 250) (8460, 4) (8460, 250, 6, 7, 1) (443, 250, 6, 7, 1)\n",
            "(443, 22, 250) (443, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CNN + GRU model with all subjects"
      ],
      "metadata": {
        "id": "IxgOEM-3WIPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Permute, Reshape, MaxPooling2D, GRU\n",
        "from keras import initializers, Model, optimizers, callbacks\n",
        "from keras import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "GswnoLTxWgFD"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Complicated Model - the same as Zhang`s\n",
        "# input_shape = (250, 6, 7, 1)\n",
        "# lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "# # TimeDistributed Wrapper\n",
        "# def timeDist(layer, prev_layer, name):\n",
        "#     return TimeDistributed(layer, name=name)(prev_layer)\n",
        "    \n",
        "# # Input layer\n",
        "# inputs = Input(shape=input_shape)\n",
        "\n",
        "# # Convolutional layers block\n",
        "# x = timeDist(Conv2D(32, (3,3), padding='same', \n",
        "#                     data_format='channels_last', kernel_initializer=lecun), inputs, name='CNN1')\n",
        "# x = BatchNormalization(name='batch1')(x)\n",
        "# x = Activation('elu', name='act1')(x)\n",
        "# x = timeDist(Conv2D(64, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN2')\n",
        "# x = BatchNormalization(name='batch2')(x)\n",
        "# x = Activation('elu', name='act2')(x)\n",
        "# x = timeDist(Conv2D(128, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN3')\n",
        "# x = BatchNormalization(name='batch3')(x)\n",
        "# x = Activation('elu', name='act3')(x)\n",
        "# x = timeDist(Flatten(), x, name='flatten')\n",
        "\n",
        "# # Fully connected layer block\n",
        "# y = Dense(64, kernel_initializer=lecun, name='FC')(x)\n",
        "# y = Dropout(0.5, name='dropout1')(y)\n",
        "# y = BatchNormalization(name='batch4')(y)\n",
        "# y = Activation(activation='elu')(y)\n",
        "\n",
        "# # Recurrent layers block\n",
        "# #z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
        "# z = LSTM(64, kernel_initializer=lecun, name='LSTM2')(y)\n",
        "\n",
        "# # Fully connected layer block\n",
        "# h = Dense(128, kernel_initializer=lecun, activation='relu', name='FC2')(z)\n",
        "# h = Dropout(0.5, name='dropout2')(h)\n",
        "\n",
        "# # Output layer\n",
        "# outputs = Dense(4, activation='softmax')(h)\n",
        "\n",
        "# # Model compile\n",
        "# model = Model(inputs=inputs, outputs=outputs)\n",
        "# model.summary()\n",
        "\n",
        "lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Permute((2, 1), input_shape=(22, 250)))\n",
        "model.add(Reshape((250, 22, 1)))\n",
        "\n",
        "model.add(Conv2D(filters=25, kernel_size=(10,1), kernel_initializer = lecun, strides=1, data_format=\"channels_last\"))\n",
        "model.add(Conv2D(filters=25, kernel_size=(1,22), kernel_initializer = lecun ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling2D(pool_size = (3,1), strides = (3,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Conv Pool Block 2\n",
        "model.add(Conv2D(filters = 50, kernel_size = (10,1), activation = 'elu', kernel_initializer = lecun))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling2D(pool_size = (3,1), strides = (3,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Permute((1, 3, 2)))\n",
        "model.add(Reshape((23, 50)))\n",
        "\n",
        "# GRU layers\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "# model.add(GRU(16, return_sequences=True))\n",
        "# model.add(GRU(16, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSm44W_mWdxt",
        "outputId": "cfe0f8de-5d5a-447d-9c45-a90ab2cf8f97"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " permute (Permute)           (None, 250, 22)           0         \n",
            "                                                                 \n",
            " reshape (Reshape)           (None, 250, 22, 1)        0         \n",
            "                                                                 \n",
            " conv2d (Conv2D)             (None, 241, 22, 25)       275       \n",
            "                                                                 \n",
            " conv2d_1 (Conv2D)           (None, 241, 1, 25)        13775     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 241, 1, 25)       100       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 241, 1, 25)        0         \n",
            "                                                                 \n",
            " max_pooling2d (MaxPooling2D  (None, 80, 1, 25)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 80, 1, 25)         0         \n",
            "                                                                 \n",
            " conv2d_2 (Conv2D)           (None, 71, 1, 50)         12550     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 71, 1, 50)        200       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 71, 1, 50)         0         \n",
            "                                                                 \n",
            " max_pooling2d_1 (MaxPooling  (None, 23, 1, 50)        0         \n",
            " 2D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 23, 1, 50)         0         \n",
            "                                                                 \n",
            " permute_1 (Permute)         (None, 23, 50, 1)         0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 23, 50)            0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 23, 16)            3264      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 23, 16)            1632      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 23, 16)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 368)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 1476      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 33,272\n",
            "Trainable params: 33,122\n",
            "Non-trainable params: 150\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(X_train_prep, y_train_prep, batch_size=64, epochs=200,\n",
        "                    validation_data=(X_valid_prep, y_valid_prep))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQNJ4-8OXRB0",
        "outputId": "f8251a25-c556-4ce5-ee1a-96ff3eb28d3e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "106/106 [==============================] - 8s 37ms/step - loss: 1.4323 - acc: 0.2835 - val_loss: 1.3405 - val_acc: 0.3475\n",
            "Epoch 2/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.3134 - acc: 0.3679 - val_loss: 1.2542 - val_acc: 0.4255\n",
            "Epoch 3/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.2488 - acc: 0.4208 - val_loss: 1.2172 - val_acc: 0.4184\n",
            "Epoch 4/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.1949 - acc: 0.4648 - val_loss: 1.1629 - val_acc: 0.4917\n",
            "Epoch 5/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 1.1446 - acc: 0.5006 - val_loss: 1.1476 - val_acc: 0.4894\n",
            "Epoch 6/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.1034 - acc: 0.5254 - val_loss: 1.1104 - val_acc: 0.5106\n",
            "Epoch 7/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.0600 - acc: 0.5523 - val_loss: 1.0807 - val_acc: 0.5296\n",
            "Epoch 8/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.0282 - acc: 0.5733 - val_loss: 1.0813 - val_acc: 0.5414\n",
            "Epoch 9/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 1.0088 - acc: 0.5884 - val_loss: 1.0520 - val_acc: 0.5414\n",
            "Epoch 10/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.9768 - acc: 0.5918 - val_loss: 1.0400 - val_acc: 0.5650\n",
            "Epoch 11/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.9539 - acc: 0.6089 - val_loss: 1.0377 - val_acc: 0.5674\n",
            "Epoch 12/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.9226 - acc: 0.6268 - val_loss: 0.9906 - val_acc: 0.5839\n",
            "Epoch 13/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.8962 - acc: 0.6392 - val_loss: 0.9895 - val_acc: 0.5910\n",
            "Epoch 14/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.8677 - acc: 0.6503 - val_loss: 0.9897 - val_acc: 0.6028\n",
            "Epoch 15/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.8582 - acc: 0.6591 - val_loss: 0.9115 - val_acc: 0.6383\n",
            "Epoch 16/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.8324 - acc: 0.6735 - val_loss: 0.9112 - val_acc: 0.6170\n",
            "Epoch 17/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.8031 - acc: 0.6828 - val_loss: 0.9942 - val_acc: 0.6123\n",
            "Epoch 18/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.7907 - acc: 0.6882 - val_loss: 0.8715 - val_acc: 0.6501\n",
            "Epoch 19/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.7742 - acc: 0.6933 - val_loss: 0.9232 - val_acc: 0.6359\n",
            "Epoch 20/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.7299 - acc: 0.7142 - val_loss: 0.8971 - val_acc: 0.6454\n",
            "Epoch 21/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.7346 - acc: 0.7089 - val_loss: 0.8788 - val_acc: 0.6714\n",
            "Epoch 22/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.7207 - acc: 0.7202 - val_loss: 0.8771 - val_acc: 0.6690\n",
            "Epoch 23/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.7122 - acc: 0.7219 - val_loss: 0.9042 - val_acc: 0.6407\n",
            "Epoch 24/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6958 - acc: 0.7333 - val_loss: 0.8425 - val_acc: 0.6596\n",
            "Epoch 25/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6880 - acc: 0.7357 - val_loss: 0.8504 - val_acc: 0.6690\n",
            "Epoch 26/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6482 - acc: 0.7509 - val_loss: 0.8371 - val_acc: 0.6998\n",
            "Epoch 27/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6442 - acc: 0.7550 - val_loss: 0.8565 - val_acc: 0.7045\n",
            "Epoch 28/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6494 - acc: 0.7510 - val_loss: 0.8573 - val_acc: 0.6761\n",
            "Epoch 29/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6447 - acc: 0.7592 - val_loss: 0.9521 - val_acc: 0.6525\n",
            "Epoch 30/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6171 - acc: 0.7603 - val_loss: 0.8194 - val_acc: 0.6832\n",
            "Epoch 31/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6280 - acc: 0.7537 - val_loss: 0.8587 - val_acc: 0.6643\n",
            "Epoch 32/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.6108 - acc: 0.7599 - val_loss: 0.9192 - val_acc: 0.6501\n",
            "Epoch 33/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5904 - acc: 0.7745 - val_loss: 0.8200 - val_acc: 0.6950\n",
            "Epoch 34/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5882 - acc: 0.7747 - val_loss: 0.8095 - val_acc: 0.6903\n",
            "Epoch 35/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5909 - acc: 0.7708 - val_loss: 0.8786 - val_acc: 0.6761\n",
            "Epoch 36/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5789 - acc: 0.7759 - val_loss: 0.7962 - val_acc: 0.6950\n",
            "Epoch 37/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5701 - acc: 0.7829 - val_loss: 0.8366 - val_acc: 0.7045\n",
            "Epoch 38/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5572 - acc: 0.7872 - val_loss: 0.8944 - val_acc: 0.6879\n",
            "Epoch 39/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.5609 - acc: 0.7793 - val_loss: 0.8271 - val_acc: 0.6927\n",
            "Epoch 40/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5505 - acc: 0.7930 - val_loss: 0.8034 - val_acc: 0.6974\n",
            "Epoch 41/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5437 - acc: 0.7866 - val_loss: 0.8490 - val_acc: 0.6903\n",
            "Epoch 42/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5269 - acc: 0.8001 - val_loss: 0.8215 - val_acc: 0.6785\n",
            "Epoch 43/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5386 - acc: 0.7942 - val_loss: 0.8760 - val_acc: 0.6667\n",
            "Epoch 44/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5263 - acc: 0.7952 - val_loss: 0.9210 - val_acc: 0.6667\n",
            "Epoch 45/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5226 - acc: 0.7993 - val_loss: 0.8449 - val_acc: 0.6950\n",
            "Epoch 46/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.5104 - acc: 0.7996 - val_loss: 0.8884 - val_acc: 0.6785\n",
            "Epoch 47/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.5094 - acc: 0.8016 - val_loss: 0.8711 - val_acc: 0.6809\n",
            "Epoch 48/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4985 - acc: 0.8116 - val_loss: 0.8899 - val_acc: 0.6643\n",
            "Epoch 49/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.4791 - acc: 0.8184 - val_loss: 0.8446 - val_acc: 0.6927\n",
            "Epoch 50/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.4801 - acc: 0.8144 - val_loss: 0.9180 - val_acc: 0.6690\n",
            "Epoch 51/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.4913 - acc: 0.8095 - val_loss: 0.9705 - val_acc: 0.6525\n",
            "Epoch 52/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4890 - acc: 0.8128 - val_loss: 0.8695 - val_acc: 0.6714\n",
            "Epoch 53/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4774 - acc: 0.8180 - val_loss: 0.9124 - val_acc: 0.6619\n",
            "Epoch 54/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4691 - acc: 0.8233 - val_loss: 0.8029 - val_acc: 0.6927\n",
            "Epoch 55/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4664 - acc: 0.8223 - val_loss: 0.9033 - val_acc: 0.6714\n",
            "Epoch 56/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4405 - acc: 0.8347 - val_loss: 0.9149 - val_acc: 0.6714\n",
            "Epoch 57/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4489 - acc: 0.8271 - val_loss: 0.8367 - val_acc: 0.6856\n",
            "Epoch 58/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4430 - acc: 0.8305 - val_loss: 0.8489 - val_acc: 0.6950\n",
            "Epoch 59/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4528 - acc: 0.8273 - val_loss: 0.8327 - val_acc: 0.7116\n",
            "Epoch 60/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4354 - acc: 0.8398 - val_loss: 0.9115 - val_acc: 0.6809\n",
            "Epoch 61/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4449 - acc: 0.8338 - val_loss: 0.8809 - val_acc: 0.6856\n",
            "Epoch 62/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.4288 - acc: 0.8378 - val_loss: 0.8640 - val_acc: 0.6998\n",
            "Epoch 63/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4254 - acc: 0.8327 - val_loss: 0.8895 - val_acc: 0.7069\n",
            "Epoch 64/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4234 - acc: 0.8432 - val_loss: 0.9331 - val_acc: 0.6856\n",
            "Epoch 65/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4331 - acc: 0.8381 - val_loss: 0.9262 - val_acc: 0.6690\n",
            "Epoch 66/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4208 - acc: 0.8416 - val_loss: 0.8257 - val_acc: 0.7021\n",
            "Epoch 67/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4163 - acc: 0.8354 - val_loss: 0.9284 - val_acc: 0.6738\n",
            "Epoch 68/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4244 - acc: 0.8400 - val_loss: 0.8105 - val_acc: 0.7069\n",
            "Epoch 69/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3977 - acc: 0.8469 - val_loss: 0.9002 - val_acc: 0.6903\n",
            "Epoch 70/200\n",
            "106/106 [==============================] - 3s 27ms/step - loss: 0.4148 - acc: 0.8453 - val_loss: 0.8287 - val_acc: 0.6856\n",
            "Epoch 71/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4021 - acc: 0.8447 - val_loss: 0.8880 - val_acc: 0.6879\n",
            "Epoch 72/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3946 - acc: 0.8515 - val_loss: 0.8896 - val_acc: 0.6879\n",
            "Epoch 73/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3874 - acc: 0.8540 - val_loss: 0.8724 - val_acc: 0.7021\n",
            "Epoch 74/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.4031 - acc: 0.8509 - val_loss: 0.8610 - val_acc: 0.6950\n",
            "Epoch 75/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.4016 - acc: 0.8475 - val_loss: 0.8679 - val_acc: 0.6974\n",
            "Epoch 76/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3963 - acc: 0.8493 - val_loss: 0.8544 - val_acc: 0.6903\n",
            "Epoch 77/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3891 - acc: 0.8509 - val_loss: 0.8468 - val_acc: 0.7021\n",
            "Epoch 78/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3886 - acc: 0.8527 - val_loss: 0.8305 - val_acc: 0.6998\n",
            "Epoch 79/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3832 - acc: 0.8555 - val_loss: 0.9145 - val_acc: 0.6832\n",
            "Epoch 80/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3789 - acc: 0.8571 - val_loss: 0.8464 - val_acc: 0.6927\n",
            "Epoch 81/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3802 - acc: 0.8585 - val_loss: 0.9125 - val_acc: 0.6785\n",
            "Epoch 82/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3871 - acc: 0.8493 - val_loss: 0.8485 - val_acc: 0.6809\n",
            "Epoch 83/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3733 - acc: 0.8605 - val_loss: 0.8911 - val_acc: 0.6809\n",
            "Epoch 84/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3721 - acc: 0.8601 - val_loss: 0.9407 - val_acc: 0.6690\n",
            "Epoch 85/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3674 - acc: 0.8651 - val_loss: 0.8920 - val_acc: 0.6879\n",
            "Epoch 86/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3752 - acc: 0.8574 - val_loss: 0.9477 - val_acc: 0.6950\n",
            "Epoch 87/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3708 - acc: 0.8610 - val_loss: 0.9105 - val_acc: 0.6785\n",
            "Epoch 88/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3599 - acc: 0.8642 - val_loss: 0.9400 - val_acc: 0.6879\n",
            "Epoch 89/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3534 - acc: 0.8684 - val_loss: 0.8736 - val_acc: 0.6950\n",
            "Epoch 90/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3503 - acc: 0.8679 - val_loss: 0.8887 - val_acc: 0.6832\n",
            "Epoch 91/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3709 - acc: 0.8626 - val_loss: 0.8517 - val_acc: 0.6974\n",
            "Epoch 92/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3583 - acc: 0.8654 - val_loss: 0.9081 - val_acc: 0.6998\n",
            "Epoch 93/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3586 - acc: 0.8633 - val_loss: 0.8698 - val_acc: 0.6974\n",
            "Epoch 94/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3467 - acc: 0.8672 - val_loss: 0.9249 - val_acc: 0.6903\n",
            "Epoch 95/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3658 - acc: 0.8604 - val_loss: 0.9002 - val_acc: 0.6832\n",
            "Epoch 96/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3561 - acc: 0.8657 - val_loss: 0.8886 - val_acc: 0.6903\n",
            "Epoch 97/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3378 - acc: 0.8719 - val_loss: 0.8981 - val_acc: 0.6950\n",
            "Epoch 98/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3430 - acc: 0.8698 - val_loss: 0.8932 - val_acc: 0.7116\n",
            "Epoch 99/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3501 - acc: 0.8670 - val_loss: 0.8976 - val_acc: 0.7021\n",
            "Epoch 100/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3398 - acc: 0.8740 - val_loss: 1.0159 - val_acc: 0.7092\n",
            "Epoch 101/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3484 - acc: 0.8682 - val_loss: 0.9592 - val_acc: 0.6927\n",
            "Epoch 102/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3270 - acc: 0.8754 - val_loss: 0.9004 - val_acc: 0.6903\n",
            "Epoch 103/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3353 - acc: 0.8750 - val_loss: 0.9133 - val_acc: 0.7069\n",
            "Epoch 104/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3475 - acc: 0.8691 - val_loss: 0.8583 - val_acc: 0.7116\n",
            "Epoch 105/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3322 - acc: 0.8757 - val_loss: 0.8848 - val_acc: 0.7021\n",
            "Epoch 106/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3377 - acc: 0.8709 - val_loss: 0.9154 - val_acc: 0.6903\n",
            "Epoch 107/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3397 - acc: 0.8728 - val_loss: 0.9418 - val_acc: 0.6927\n",
            "Epoch 108/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3423 - acc: 0.8728 - val_loss: 0.8860 - val_acc: 0.7069\n",
            "Epoch 109/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3184 - acc: 0.8777 - val_loss: 0.9334 - val_acc: 0.6927\n",
            "Epoch 110/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3179 - acc: 0.8805 - val_loss: 0.9002 - val_acc: 0.6809\n",
            "Epoch 111/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3245 - acc: 0.8783 - val_loss: 0.8807 - val_acc: 0.7116\n",
            "Epoch 112/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3126 - acc: 0.8805 - val_loss: 0.9007 - val_acc: 0.6856\n",
            "Epoch 113/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3127 - acc: 0.8842 - val_loss: 0.9005 - val_acc: 0.6974\n",
            "Epoch 114/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3226 - acc: 0.8805 - val_loss: 0.9933 - val_acc: 0.6998\n",
            "Epoch 115/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3330 - acc: 0.8754 - val_loss: 0.8872 - val_acc: 0.7092\n",
            "Epoch 116/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3238 - acc: 0.8853 - val_loss: 0.8807 - val_acc: 0.7045\n",
            "Epoch 117/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3185 - acc: 0.8774 - val_loss: 0.8798 - val_acc: 0.6903\n",
            "Epoch 118/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3250 - acc: 0.8787 - val_loss: 0.9396 - val_acc: 0.7092\n",
            "Epoch 119/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3190 - acc: 0.8787 - val_loss: 0.8920 - val_acc: 0.7092\n",
            "Epoch 120/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3225 - acc: 0.8766 - val_loss: 0.9239 - val_acc: 0.6927\n",
            "Epoch 121/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3156 - acc: 0.8797 - val_loss: 0.9273 - val_acc: 0.7045\n",
            "Epoch 122/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3139 - acc: 0.8837 - val_loss: 0.8860 - val_acc: 0.7092\n",
            "Epoch 123/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3098 - acc: 0.8843 - val_loss: 1.0226 - val_acc: 0.6738\n",
            "Epoch 124/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3125 - acc: 0.8859 - val_loss: 0.9732 - val_acc: 0.6974\n",
            "Epoch 125/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3081 - acc: 0.8843 - val_loss: 0.9429 - val_acc: 0.7116\n",
            "Epoch 126/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3063 - acc: 0.8862 - val_loss: 0.9481 - val_acc: 0.7069\n",
            "Epoch 127/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3055 - acc: 0.8864 - val_loss: 0.9802 - val_acc: 0.6879\n",
            "Epoch 128/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3106 - acc: 0.8842 - val_loss: 0.9651 - val_acc: 0.7116\n",
            "Epoch 129/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3065 - acc: 0.8867 - val_loss: 0.8964 - val_acc: 0.7092\n",
            "Epoch 130/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2902 - acc: 0.8927 - val_loss: 0.9454 - val_acc: 0.6998\n",
            "Epoch 131/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3023 - acc: 0.8898 - val_loss: 0.8475 - val_acc: 0.7305\n",
            "Epoch 132/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2952 - acc: 0.8932 - val_loss: 0.9306 - val_acc: 0.7116\n",
            "Epoch 133/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3025 - acc: 0.8883 - val_loss: 0.9063 - val_acc: 0.7139\n",
            "Epoch 134/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2987 - acc: 0.8904 - val_loss: 0.8988 - val_acc: 0.7092\n",
            "Epoch 135/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.3020 - acc: 0.8871 - val_loss: 0.9133 - val_acc: 0.7116\n",
            "Epoch 136/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2990 - acc: 0.8895 - val_loss: 0.9891 - val_acc: 0.6950\n",
            "Epoch 137/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2938 - acc: 0.8898 - val_loss: 0.9432 - val_acc: 0.6998\n",
            "Epoch 138/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2954 - acc: 0.8944 - val_loss: 0.9784 - val_acc: 0.7021\n",
            "Epoch 139/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2982 - acc: 0.8874 - val_loss: 0.9042 - val_acc: 0.7092\n",
            "Epoch 140/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2926 - acc: 0.8901 - val_loss: 0.9787 - val_acc: 0.6927\n",
            "Epoch 141/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.3020 - acc: 0.8864 - val_loss: 0.9368 - val_acc: 0.7092\n",
            "Epoch 142/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2950 - acc: 0.8883 - val_loss: 0.9571 - val_acc: 0.6950\n",
            "Epoch 143/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2927 - acc: 0.8933 - val_loss: 0.9411 - val_acc: 0.6974\n",
            "Epoch 144/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2895 - acc: 0.8923 - val_loss: 0.9576 - val_acc: 0.7116\n",
            "Epoch 145/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2810 - acc: 0.8936 - val_loss: 0.9069 - val_acc: 0.6879\n",
            "Epoch 146/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2834 - acc: 0.8935 - val_loss: 0.9776 - val_acc: 0.6856\n",
            "Epoch 147/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2860 - acc: 0.8942 - val_loss: 0.9906 - val_acc: 0.6856\n",
            "Epoch 148/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2867 - acc: 0.8927 - val_loss: 1.0166 - val_acc: 0.6927\n",
            "Epoch 149/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2817 - acc: 0.8969 - val_loss: 0.9454 - val_acc: 0.7187\n",
            "Epoch 150/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2869 - acc: 0.8957 - val_loss: 0.9799 - val_acc: 0.6974\n",
            "Epoch 151/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2827 - acc: 0.8964 - val_loss: 0.9582 - val_acc: 0.7069\n",
            "Epoch 152/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2770 - acc: 0.8969 - val_loss: 0.9489 - val_acc: 0.6974\n",
            "Epoch 153/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2817 - acc: 0.8951 - val_loss: 1.0017 - val_acc: 0.7021\n",
            "Epoch 154/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2903 - acc: 0.8930 - val_loss: 0.9071 - val_acc: 0.7045\n",
            "Epoch 155/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2840 - acc: 0.8949 - val_loss: 0.9936 - val_acc: 0.7069\n",
            "Epoch 156/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2745 - acc: 0.8992 - val_loss: 0.9605 - val_acc: 0.6998\n",
            "Epoch 157/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2874 - acc: 0.8930 - val_loss: 0.9268 - val_acc: 0.7092\n",
            "Epoch 158/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2739 - acc: 0.9007 - val_loss: 1.0304 - val_acc: 0.6903\n",
            "Epoch 159/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2745 - acc: 0.8915 - val_loss: 0.9674 - val_acc: 0.7045\n",
            "Epoch 160/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2686 - acc: 0.8985 - val_loss: 0.9649 - val_acc: 0.7069\n",
            "Epoch 161/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2780 - acc: 0.8979 - val_loss: 0.9681 - val_acc: 0.7092\n",
            "Epoch 162/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2593 - acc: 0.9017 - val_loss: 0.8900 - val_acc: 0.7187\n",
            "Epoch 163/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2653 - acc: 0.9017 - val_loss: 0.9185 - val_acc: 0.7069\n",
            "Epoch 164/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2716 - acc: 0.8957 - val_loss: 0.9306 - val_acc: 0.7045\n",
            "Epoch 165/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2698 - acc: 0.8966 - val_loss: 0.9503 - val_acc: 0.7116\n",
            "Epoch 166/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2673 - acc: 0.9053 - val_loss: 0.9648 - val_acc: 0.7045\n",
            "Epoch 167/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2674 - acc: 0.9013 - val_loss: 0.9979 - val_acc: 0.6927\n",
            "Epoch 168/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2737 - acc: 0.8964 - val_loss: 0.9238 - val_acc: 0.7376\n",
            "Epoch 169/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2710 - acc: 0.8975 - val_loss: 0.9498 - val_acc: 0.7281\n",
            "Epoch 170/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2603 - acc: 0.8994 - val_loss: 0.9444 - val_acc: 0.7258\n",
            "Epoch 171/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2659 - acc: 0.8969 - val_loss: 0.9407 - val_acc: 0.7069\n",
            "Epoch 172/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2629 - acc: 0.9078 - val_loss: 0.9435 - val_acc: 0.7045\n",
            "Epoch 173/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2735 - acc: 0.8969 - val_loss: 1.0042 - val_acc: 0.7021\n",
            "Epoch 174/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2662 - acc: 0.9003 - val_loss: 0.9200 - val_acc: 0.7210\n",
            "Epoch 175/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2724 - acc: 0.9014 - val_loss: 0.9568 - val_acc: 0.7116\n",
            "Epoch 176/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2724 - acc: 0.8979 - val_loss: 0.9756 - val_acc: 0.7021\n",
            "Epoch 177/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2624 - acc: 0.8991 - val_loss: 0.9499 - val_acc: 0.7139\n",
            "Epoch 178/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2581 - acc: 0.9035 - val_loss: 0.9757 - val_acc: 0.7187\n",
            "Epoch 179/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2715 - acc: 0.8942 - val_loss: 0.9622 - val_acc: 0.7116\n",
            "Epoch 180/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2680 - acc: 0.8994 - val_loss: 0.9246 - val_acc: 0.7092\n",
            "Epoch 181/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2676 - acc: 0.9037 - val_loss: 1.0508 - val_acc: 0.6903\n",
            "Epoch 182/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2538 - acc: 0.9006 - val_loss: 0.9585 - val_acc: 0.7069\n",
            "Epoch 183/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2503 - acc: 0.9082 - val_loss: 1.0749 - val_acc: 0.6927\n",
            "Epoch 184/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2442 - acc: 0.9084 - val_loss: 1.0100 - val_acc: 0.7092\n",
            "Epoch 185/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2586 - acc: 0.9066 - val_loss: 1.0089 - val_acc: 0.6927\n",
            "Epoch 186/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2573 - acc: 0.9025 - val_loss: 0.9851 - val_acc: 0.6998\n",
            "Epoch 187/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2662 - acc: 0.8970 - val_loss: 1.0043 - val_acc: 0.7163\n",
            "Epoch 188/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2615 - acc: 0.9025 - val_loss: 0.9936 - val_acc: 0.7092\n",
            "Epoch 189/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2745 - acc: 0.8975 - val_loss: 0.9627 - val_acc: 0.7139\n",
            "Epoch 190/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2639 - acc: 0.9001 - val_loss: 0.9595 - val_acc: 0.7281\n",
            "Epoch 191/200\n",
            "106/106 [==============================] - 3s 28ms/step - loss: 0.2769 - acc: 0.8951 - val_loss: 1.0052 - val_acc: 0.6998\n",
            "Epoch 192/200\n",
            "106/106 [==============================] - 3s 30ms/step - loss: 0.2483 - acc: 0.9040 - val_loss: 1.0872 - val_acc: 0.6998\n",
            "Epoch 193/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2490 - acc: 0.9062 - val_loss: 0.9777 - val_acc: 0.7163\n",
            "Epoch 194/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2597 - acc: 0.9019 - val_loss: 0.9431 - val_acc: 0.7163\n",
            "Epoch 195/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2375 - acc: 0.9152 - val_loss: 0.9661 - val_acc: 0.7092\n",
            "Epoch 196/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2388 - acc: 0.9119 - val_loss: 1.0342 - val_acc: 0.7139\n",
            "Epoch 197/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2425 - acc: 0.9059 - val_loss: 1.0351 - val_acc: 0.6950\n",
            "Epoch 198/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2625 - acc: 0.9032 - val_loss: 1.0394 - val_acc: 0.6879\n",
            "Epoch 199/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2533 - acc: 0.9081 - val_loss: 0.9904 - val_acc: 0.7045\n",
            "Epoch 200/200\n",
            "106/106 [==============================] - 3s 29ms/step - loss: 0.2551 - acc: 0.9051 - val_loss: 1.1041 - val_acc: 0.6738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#model = keras.models.load_model(\"best_model.h5\")\n",
        "\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5W7-Qi6vG_9",
        "outputId": "0aa57c5e-4756-4190-8416-0c29836050f0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 11ms/step - loss: 1.0711 - acc: 0.6975\n",
            "Test accuracy 0.697516918182373\n",
            "Test loss 1.0711007118225098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'], label='train')\n",
        "plt.plot(history.history['val_acc'], label='test')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qS3hSLpYWYAa",
        "outputId": "f09baf9e-df2f-4028-ef82-034e0226d39a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model on entire data"
      ],
      "metadata": {
        "id": "quT1aOWLWaS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all,y_train_all= data_prep(X_train_valid,y_train_valid,2,2,True)"
      ],
      "metadata": {
        "id": "CymOAvmyeko3",
        "outputId": "66324a79-5bc7-4d61-d9a4-05ae3dd24b6f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (2115, 22, 500)\n",
            "Shape of X after maxpooling: (2115, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (8460, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_all = keras.utils.to_categorical(y_train_all, num_classes)"
      ],
      "metadata": {
        "id": "d-xe5hCKexlM"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.00005)"
      ],
      "metadata": {
        "id": "fbNufw_96wwT"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "history = model.fit(X_train_all, y_train_all, batch_size=64, epochs=100)"
      ],
      "metadata": {
        "id": "B6E65haLe7VH",
        "outputId": "6179d685-0ed7-4d1c-8427-af56cf2f359d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "133/133 [==============================] - 7s 26ms/step - loss: 0.1677 - acc: 0.9404\n",
            "Epoch 2/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1496 - acc: 0.9437\n",
            "Epoch 3/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1545 - acc: 0.9436\n",
            "Epoch 4/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1553 - acc: 0.9427\n",
            "Epoch 5/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1561 - acc: 0.9423\n",
            "Epoch 6/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1597 - acc: 0.9410\n",
            "Epoch 7/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1606 - acc: 0.9382\n",
            "Epoch 8/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1530 - acc: 0.9452\n",
            "Epoch 9/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1554 - acc: 0.9420\n",
            "Epoch 10/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1582 - acc: 0.9410\n",
            "Epoch 11/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1673 - acc: 0.9421\n",
            "Epoch 12/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1520 - acc: 0.9449\n",
            "Epoch 13/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1505 - acc: 0.9427\n",
            "Epoch 14/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1456 - acc: 0.9468\n",
            "Epoch 15/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1547 - acc: 0.9436\n",
            "Epoch 16/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1607 - acc: 0.9416\n",
            "Epoch 17/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1518 - acc: 0.9460\n",
            "Epoch 18/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1522 - acc: 0.9453\n",
            "Epoch 19/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1489 - acc: 0.9440\n",
            "Epoch 20/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1534 - acc: 0.9467\n",
            "Epoch 21/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1613 - acc: 0.9392\n",
            "Epoch 22/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1575 - acc: 0.9431\n",
            "Epoch 23/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1558 - acc: 0.9441\n",
            "Epoch 24/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1504 - acc: 0.9469\n",
            "Epoch 25/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1606 - acc: 0.9410\n",
            "Epoch 26/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1537 - acc: 0.9440\n",
            "Epoch 27/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1583 - acc: 0.9417\n",
            "Epoch 28/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1502 - acc: 0.9436\n",
            "Epoch 29/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1606 - acc: 0.9429\n",
            "Epoch 30/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1435 - acc: 0.9461\n",
            "Epoch 31/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1634 - acc: 0.9407\n",
            "Epoch 32/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1404 - acc: 0.9498\n",
            "Epoch 33/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1465 - acc: 0.9487\n",
            "Epoch 34/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1475 - acc: 0.9465\n",
            "Epoch 35/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1499 - acc: 0.9462\n",
            "Epoch 36/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1523 - acc: 0.9455\n",
            "Epoch 37/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1532 - acc: 0.9453\n",
            "Epoch 38/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1588 - acc: 0.9435\n",
            "Epoch 39/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1475 - acc: 0.9487\n",
            "Epoch 40/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1548 - acc: 0.9422\n",
            "Epoch 41/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1527 - acc: 0.9469\n",
            "Epoch 42/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1479 - acc: 0.9439\n",
            "Epoch 43/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1542 - acc: 0.9422\n",
            "Epoch 44/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1548 - acc: 0.9414\n",
            "Epoch 45/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1453 - acc: 0.9468\n",
            "Epoch 46/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1655 - acc: 0.9395\n",
            "Epoch 47/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1507 - acc: 0.9466\n",
            "Epoch 48/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1540 - acc: 0.9444\n",
            "Epoch 49/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1587 - acc: 0.9418\n",
            "Epoch 50/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1565 - acc: 0.9434\n",
            "Epoch 51/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1521 - acc: 0.9463\n",
            "Epoch 52/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1488 - acc: 0.9465\n",
            "Epoch 53/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1560 - acc: 0.9428\n",
            "Epoch 54/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1553 - acc: 0.9433\n",
            "Epoch 55/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1576 - acc: 0.9426\n",
            "Epoch 56/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1569 - acc: 0.9431\n",
            "Epoch 57/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1572 - acc: 0.9420\n",
            "Epoch 58/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1568 - acc: 0.9418\n",
            "Epoch 59/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1573 - acc: 0.9423\n",
            "Epoch 60/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1717 - acc: 0.9396\n",
            "Epoch 61/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1559 - acc: 0.9423\n",
            "Epoch 62/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1517 - acc: 0.9452\n",
            "Epoch 63/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1468 - acc: 0.9435\n",
            "Epoch 64/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1517 - acc: 0.9455\n",
            "Epoch 65/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1478 - acc: 0.9465\n",
            "Epoch 66/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1512 - acc: 0.9476\n",
            "Epoch 67/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1619 - acc: 0.9428\n",
            "Epoch 68/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1495 - acc: 0.9447\n",
            "Epoch 69/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1501 - acc: 0.9456\n",
            "Epoch 70/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1530 - acc: 0.9429\n",
            "Epoch 71/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1641 - acc: 0.9403\n",
            "Epoch 72/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1442 - acc: 0.9453\n",
            "Epoch 73/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1549 - acc: 0.9442\n",
            "Epoch 74/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1571 - acc: 0.9402\n",
            "Epoch 75/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1591 - acc: 0.9440\n",
            "Epoch 76/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1464 - acc: 0.9474\n",
            "Epoch 77/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1524 - acc: 0.9468\n",
            "Epoch 78/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1522 - acc: 0.9443\n",
            "Epoch 79/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1521 - acc: 0.9444\n",
            "Epoch 80/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1594 - acc: 0.9437\n",
            "Epoch 81/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1614 - acc: 0.9424\n",
            "Epoch 82/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1573 - acc: 0.9430\n",
            "Epoch 83/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1526 - acc: 0.9408\n",
            "Epoch 84/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1534 - acc: 0.9443\n",
            "Epoch 85/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1509 - acc: 0.9469\n",
            "Epoch 86/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1534 - acc: 0.9427\n",
            "Epoch 87/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1444 - acc: 0.9468\n",
            "Epoch 88/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1538 - acc: 0.9442\n",
            "Epoch 89/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1587 - acc: 0.9424\n",
            "Epoch 90/100\n",
            "133/133 [==============================] - 4s 27ms/step - loss: 0.1498 - acc: 0.9475\n",
            "Epoch 91/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1600 - acc: 0.9408\n",
            "Epoch 92/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1493 - acc: 0.9441\n",
            "Epoch 93/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1508 - acc: 0.9433\n",
            "Epoch 94/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1490 - acc: 0.9447\n",
            "Epoch 95/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1516 - acc: 0.9459\n",
            "Epoch 96/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1590 - acc: 0.9442\n",
            "Epoch 97/100\n",
            "133/133 [==============================] - 4s 26ms/step - loss: 0.1552 - acc: 0.9428\n",
            "Epoch 98/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1478 - acc: 0.9469\n",
            "Epoch 99/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1557 - acc: 0.9440\n",
            "Epoch 100/100\n",
            "133/133 [==============================] - 3s 26ms/step - loss: 0.1523 - acc: 0.9427\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "SXr2-Xr1zDMU"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)\n"
      ],
      "metadata": {
        "id": "O2GM7stifF3f",
        "outputId": "3e824985-39ff-4d9b-80f7-252f8ad427e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 1s 10ms/step - loss: 1.0725 - acc: 0.7427\n",
            "Test accuracy 0.7426636815071106\n",
            "Test loss 1.072490930557251\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "APhnh7owuZGx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}