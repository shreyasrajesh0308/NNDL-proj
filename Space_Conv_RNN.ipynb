{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Space Conv - RNN.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasrajesh0308/NNDL-proj/blob/main/Space_Conv_RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Paper: "
      ],
      "metadata": {
        "id": "Wy-ORg0nyKtV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data \n",
        "\n",
        "Load preprocessed data"
      ],
      "metadata": {
        "id": "5Fl4uJtrAqU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Scu17GL9A7i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72bae2b2-06cc-4903-9dd1-ef6a626eefdf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier"
      ],
      "metadata": {
        "id": "2gXQ7pXTIqqT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_test = np.load(\"/content/drive/MyDrive/eeg_project/X_test.npy\")\n",
        "y_test = np.load(\"/content/drive/MyDrive/eeg_project/y_test.npy\")\n",
        "person_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/y_train_valid.npy\")\n",
        "person_test = np.load(\"/content/drive/MyDrive/eeg_project/person_test.npy\")\n"
      ],
      "metadata": {
        "id": "foTd2q3JIreb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n"
      ],
      "metadata": {
        "id": "g6VAQSVEUtIW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc82fa34-6caf-4b43-be91-b8b9d4ca70dc"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train_valid))\n",
        "print(np.unique(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bhxR60r5IzuR",
        "outputId": "de05e970-4c9d-4ada-e46b-91ec7a53ce08"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[769 770 771 772]\n",
            "[769 770 771 772]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "y_train_valid = y_train_valid-769\n",
        "y_test = y_test-769"
      ],
      "metadata": {
        "id": "U4K1wzmfI4Rb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(X,y,sub_sample,average,noise):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    \n",
        "    # Averaging + noise \n",
        "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
        "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
        "    \n",
        "    total_X = np.vstack((total_X, X_average))\n",
        "    total_y = np.hstack((total_y, y))\n",
        "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
        "    \n",
        "    # Subsampling\n",
        "    \n",
        "    for i in range(sub_sample):\n",
        "        \n",
        "        X_subsample = X[:, :, i::sub_sample] + \\\n",
        "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
        "            \n",
        "        total_X = np.vstack((total_X, X_subsample))\n",
        "        total_y = np.hstack((total_y, y))\n",
        "        \n",
        "    \n",
        "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_train_valid_prep,y_train_valid_prep = data_prep(X_train_valid,y_train_valid,2,2,True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-ubjiWDpR_W",
        "outputId": "d493617f-64de-4184-848e-3e157f7a40b9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (2115, 22, 500)\n",
            "Shape of X after maxpooling: (2115, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (8460, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep_test(X,y, sub_sample=2):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_test,y_test = data_prep_test(X_test, y_test,2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IhtalewhtI6R",
        "outputId": "475d856e-5188-48fe-fa1a-0eafbe69f92b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (443, 22, 500)\n",
            "Shape of X after maxpooling: (443, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def spatial_prep(X):\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  X = np.swapaxes(X, 0, 1)\n",
        "  # X = np.swapaxes(X, 1, 2)\n",
        "\n",
        "  sh = (X.shape[1],X.shape[2])\n",
        "  new_X = np.array([[np.zeros(sh), np.zeros(sh), np.zeros(sh), X[0,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), X[1,:,:], X[2,:,:], X[3,:,:], X[4,:,:], X[5,:,:], np.zeros(sh)],\n",
        "                [X[6,:,:],X[7,:,:],X[8,:,:],X[9,:,:],X[10,:,:],X[11,:,:],X[12,:,:]],\n",
        "                [np.zeros(sh), X[13,:,:],X[14,:,:],X[15,:,:],X[16,:,:],X[17,:,:], np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), X[18,:,:],X[19,:,:],X[20,:,:], np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), np.zeros(sh), X[21,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)]])\n",
        "\n",
        "  # for x_seq in X:\n",
        "  #   n = len(X)\n",
        "  #   # new_seq_X = np.array([np.array([np.zeros(n), np.zeros(n), np.zeros(n), np.array(x_seq[0]), np.zeros(n), np.zeros(n), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.array(x_seq[1]), np.array(x_seq[2]), np.array(x_seq[3]), np.array(x_seq[4]), np.array(x_seq[5]), np.zeros(n)]),\n",
        "  #   #              np.array([np.array(x_seq[6]), np.array(x_seq[7]), np.array(x_seq[8]), np.array(x_seq[9]), np.array(x_seq[10]), np.array(x_seq[11]), np.array(x_seq[12])]),\n",
        "  #   #              np.array([np.zeros(n), np.array(x_seq[13]), np.array(x_seq[14]), np.array(x_seq[15]), np.array(x_seq[16]), np.array(x_seq[17]), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.zeros(n), np.array(x_seq[18]), np.array(x_seq[19]), np.array(x_seq[20]), np.zeros(n), np.zeros(n)]),\n",
        "  #   #              np.array([np.zeros(n), np.zeros(n), np.zeros(n), np.array(x_seq[21]), np.zeros(n), np.zeros(n), np.zeros(n)])])\n",
        "  #   # new_X.append(new_seq_X)\n",
        "  #   new_seq_X = np.array([[[0]*n, [0]*n, [0]*n, x_seq[0], [0]*n, [0]*n, [0]*n ],\n",
        "  #                [[0]*n, x_seq[1], x_seq[2], x_seq[3], x_seq[4], x_seq[5], [0]*n ],\n",
        "  #                [x_seq[6], x_seq[7], x_seq[8], x_seq[9], x_seq[10], x_seq[11], x_seq[12]],\n",
        "  #                [[0]*n, x_seq[13], x_seq[14], x_seq[15], x_seq[16], x_seq[17], [0]*n],\n",
        "  #                [[0]*n, [0]*n, x_seq[18], x_seq[19], x_seq[20], [0]*n, [0]*n],\n",
        "  #                [[0]*n, [0]*n, [0]*n, x_seq[21], [0]*n, [0]*n, [0]*n]])\n",
        "  #   new_X.append(new_seq_X)\n",
        "\n",
        "  # new_X = np.array(new_X)\n",
        "  new_X = np.swapaxes(new_X, 0,2)\n",
        "  new_X = np.swapaxes(new_X, 1,2)\n",
        "  return new_X"
      ],
      "metadata": {
        "id": "-u8tro1jwuon"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep = spatial_prep(X_train_valid_prep)"
      ],
      "metadata": {
        "id": "flRuB5gPygqF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_test = spatial_prep(X_test)"
      ],
      "metadata": {
        "id": "spWmKQWT3qd1"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_spatial_prep.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l3rD3lYUyumZ",
        "outputId": "d6fc0454-8d33-49ff-8505-394e2d2ebe22"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 6, 7, 250) (443, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train_valid_prep = keras.utils.to_categorical(y_train_valid_prep, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "NZWq7ZCFq5z9"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_valid_prep.shape,y_train_valid_prep.shape, X_spatial_prep.shape, X_spatial_test.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFZfw6CuqeBs",
        "outputId": "e5c25bed-d155-4165-9f3f-680c0e160a75"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 22, 250) (8460, 4) (8460, 6, 7, 250) (443, 6, 7, 250)\n",
            "(443, 22, 250) (443, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep  = np.swapaxes(X_spatial_prep , 1,3)\n",
        "X_spatial_prep = np.swapaxes(X_spatial_prep , 2,3)\n",
        "X_spatial_test  = np.swapaxes(X_spatial_test , 1,3)\n",
        "X_spatial_test = np.swapaxes(X_spatial_test , 2,3)"
      ],
      "metadata": {
        "id": "b3_pN5eAY1bg"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_prep = X_spatial_prep.reshape(*X_spatial_prep.shape, 1)\n",
        "X_spatial_test = X_spatial_test.reshape(*X_spatial_test.shape, 1)"
      ],
      "metadata": {
        "id": "FeiaaX6qX3-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_valid_prep.shape,y_train_valid_prep.shape, X_spatial_prep.shape, X_spatial_test.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1MC6IQn7YEF-",
        "outputId": "a7c9e055-1bd6-435e-caca-d711a5f8459a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 22, 250) (8460, 4) (8460, 250, 6, 7) (443, 250, 6, 7)\n",
            "(443, 22, 250) (443, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cascade model with all subjects"
      ],
      "metadata": {
        "id": "IxgOEM-3WIPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import Conv2D,Conv3D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Permute, Reshape, MaxPooling2D,MaxPooling3D, GRU\n",
        "from keras import initializers, Model, optimizers, callbacks\n",
        "from keras import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "GswnoLTxWgFD"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ## Complicated Model - the same as Zhang`s\n",
        "# input_shape = (250, 6, 7, 1)\n",
        "# lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "# # TimeDistributed Wrapper\n",
        "# def timeDist(layer, prev_layer, name):\n",
        "#     return TimeDistributed(layer, name=name)(prev_layer)\n",
        "    \n",
        "# # Input layer\n",
        "# inputs = Input(shape=input_shape)\n",
        "\n",
        "# # Convolutional layers block\n",
        "# x = timeDist(Conv2D(32, (3,3), padding='same', \n",
        "#                     data_format='channels_last', kernel_initializer=lecun), inputs, name='CNN1')\n",
        "# x = BatchNormalization(name='batch1')(x)\n",
        "# x = Activation('elu', name='act1')(x)\n",
        "# x = timeDist(Conv2D(64, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN2')\n",
        "# x = BatchNormalization(name='batch2')(x)\n",
        "# x = Activation('elu', name='act2')(x)\n",
        "# x = timeDist(Conv2D(128, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN3')\n",
        "# x = BatchNormalization(name='batch3')(x)\n",
        "# x = Activation('elu', name='act3')(x)\n",
        "# x = timeDist(Flatten(), x, name='flatten')\n",
        "\n",
        "# # Fully connected layer block\n",
        "# y = Dense(256, kernel_initializer=lecun, name='FC')(x)\n",
        "# y = Dropout(0.5, name='dropout1')(y)\n",
        "# y = BatchNormalization(name='batch4')(y)\n",
        "# y = Activation(activation='elu')(y)\n",
        "\n",
        "# # Recurrent layers block\n",
        "# #z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
        "# z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM2')(y)\n",
        "# z = Flatten()(z)\n",
        "# # Fully connected layer block\n",
        "# h = Dense(128, kernel_initializer=lecun, activation='relu', name='FC2')(z)\n",
        "# h = Dropout(0.5, name='dropout2')(h)\n",
        "\n",
        "# # Output layer\n",
        "# outputs = Dense(4, activation='softmax')(h)\n",
        "\n",
        "# # Model compile\n",
        "# model = Model(inputs=inputs, outputs=outputs)\n",
        "# model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSm44W_mWdxt",
        "outputId": "e00437d2-7bb1-4638-fe03-9529c76fd503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 250, 6, 7, 1)]    0         \n",
            "                                                                 \n",
            " CNN1 (TimeDistributed)      (None, 250, 6, 7, 32)     320       \n",
            "                                                                 \n",
            " batch1 (BatchNormalization)  (None, 250, 6, 7, 32)    128       \n",
            "                                                                 \n",
            " act1 (Activation)           (None, 250, 6, 7, 32)     0         \n",
            "                                                                 \n",
            " CNN2 (TimeDistributed)      (None, 250, 6, 7, 64)     18496     \n",
            "                                                                 \n",
            " batch2 (BatchNormalization)  (None, 250, 6, 7, 64)    256       \n",
            "                                                                 \n",
            " act2 (Activation)           (None, 250, 6, 7, 64)     0         \n",
            "                                                                 \n",
            " CNN3 (TimeDistributed)      (None, 250, 6, 7, 128)    73856     \n",
            "                                                                 \n",
            " batch3 (BatchNormalization)  (None, 250, 6, 7, 128)   512       \n",
            "                                                                 \n",
            " act3 (Activation)           (None, 250, 6, 7, 128)    0         \n",
            "                                                                 \n",
            " flatten (TimeDistributed)   (None, 250, 5376)         0         \n",
            "                                                                 \n",
            " FC (Dense)                  (None, 250, 256)          1376512   \n",
            "                                                                 \n",
            " dropout1 (Dropout)          (None, 250, 256)          0         \n",
            "                                                                 \n",
            " batch4 (BatchNormalization)  (None, 250, 256)         1024      \n",
            "                                                                 \n",
            " activation_3 (Activation)   (None, 250, 256)          0         \n",
            "                                                                 \n",
            " LSTM2 (LSTM)                (None, 250, 64)           82176     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 16000)             0         \n",
            "                                                                 \n",
            " FC2 (Dense)                 (None, 128)               2048128   \n",
            "                                                                 \n",
            " dropout2 (Dropout)          (None, 128)               0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 4)                 516       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,601,924\n",
            "Trainable params: 3,600,964\n",
            "Non-trainable params: 960\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Complicated Model - the same as Zhang`s\n",
        "# input_shape = (250, 6, 7, 1)\n",
        "# lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "# # TimeDistributed Wrapper\n",
        "# def timeDist(layer, prev_layer, name):\n",
        "#     return TimeDistributed(layer, name=name)(prev_layer)\n",
        "    \n",
        "# # Input layer\n",
        "# inputs = Input(shape=input_shape)\n",
        "\n",
        "# # Convolutional layers block\n",
        "# x = timeDist(Conv2D(32, (3,3), padding='same', \n",
        "#                     data_format='channels_last', kernel_initializer=lecun), inputs, name='CNN1')\n",
        "# x = BatchNormalization(name='batch1')(x)\n",
        "# x = Activation('elu', name='act1')(x)\n",
        "# x = timeDist(Conv2D(64, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN2')\n",
        "# x = BatchNormalization(name='batch2')(x)\n",
        "# x = Activation('elu', name='act2')(x)\n",
        "# x = timeDist(Conv2D(128, (3,3), padding='same', data_format='channels_last', kernel_initializer=lecun), x, name='CNN3')\n",
        "# x = BatchNormalization(name='batch3')(x)\n",
        "# x = Activation('elu', name='act3')(x)\n",
        "# x = timeDist(Flatten(), x, name='flatten')\n",
        "\n",
        "# # Fully connected layer block\n",
        "# y = Dense(64, kernel_initializer=lecun, name='FC')(x)\n",
        "# y = Dropout(0.5, name='dropout1')(y)\n",
        "# y = BatchNormalization(name='batch4')(y)\n",
        "# y = Activation(activation='elu')(y)\n",
        "\n",
        "# # Recurrent layers block\n",
        "# #z = LSTM(64, kernel_initializer=lecun, return_sequences=True, name='LSTM1')(y)\n",
        "# z = LSTM(64, kernel_initializer=lecun, name='LSTM2')(y)\n",
        "\n",
        "# # Fully connected layer block\n",
        "# h = Dense(128, kernel_initializer=lecun, activation='relu', name='FC2')(z)\n",
        "# h = Dropout(0.5, name='dropout2')(h)\n",
        "\n",
        "# # Output layer\n",
        "# outputs = Dense(4, activation='softmax')(h)\n",
        "\n",
        "# # Model compile\n",
        "# model = Model(inputs=inputs, outputs=outputs)\n",
        "# model.summary()\n",
        "\n",
        "lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "# model.add(Permute((2, 1), input_shape=(6,7, 250)))\n",
        "# model.add(Permute((3, 1), input_shape=(6,7, 250)))\n",
        "model.add(Reshape((250, 6, 7, 1), input_shape = (250,6,7)))\n",
        "\n",
        "model.add(Conv3D(filters=25, kernel_size=(10,1,1), kernel_initializer = lecun, strides=1, data_format=\"channels_last\"))\n",
        "model.add(Conv3D(filters=25, kernel_size=(1,6,7), kernel_initializer = lecun ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling3D(pool_size = (3,1,1), strides = (3,1,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Conv Pool Block 2\n",
        "model.add(Conv3D(filters = 50, kernel_size = (10,1,1), activation = 'elu', kernel_initializer = lecun))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling3D(pool_size = (3,1,1), strides = (3,1,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Permute((1, 4, 3, 2)))\n",
        "model.add(Reshape((23, 50)))\n",
        "\n",
        "# GRU layers\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "# model.add(GRU(16, return_sequences=True))\n",
        "# model.add(GRU(16, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4VeaUcZoQP2E",
        "outputId": "f6f151b8-28c6-4940-f49e-1c0e27ae65ce"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape (Reshape)           (None, 250, 6, 7, 1)      0         \n",
            "                                                                 \n",
            " conv3d (Conv3D)             (None, 241, 6, 7, 25)     275       \n",
            "                                                                 \n",
            " conv3d_1 (Conv3D)           (None, 241, 1, 1, 25)     26275     \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 241, 1, 1, 25)    100       \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " activation (Activation)     (None, 241, 1, 1, 25)     0         \n",
            "                                                                 \n",
            " max_pooling3d (MaxPooling3D  (None, 80, 1, 1, 25)     0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 80, 1, 1, 25)      0         \n",
            "                                                                 \n",
            " conv3d_2 (Conv3D)           (None, 71, 1, 1, 50)      12550     \n",
            "                                                                 \n",
            " batch_normalization_1 (Batc  (None, 71, 1, 1, 50)     200       \n",
            " hNormalization)                                                 \n",
            "                                                                 \n",
            " activation_1 (Activation)   (None, 71, 1, 1, 50)      0         \n",
            "                                                                 \n",
            " max_pooling3d_1 (MaxPooling  (None, 23, 1, 1, 50)     0         \n",
            " 3D)                                                             \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 23, 1, 1, 50)      0         \n",
            "                                                                 \n",
            " permute (Permute)           (None, 23, 50, 1, 1)      0         \n",
            "                                                                 \n",
            " reshape_1 (Reshape)         (None, 23, 50)            0         \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 23, 16)            3264      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (None, 23, 16)            1632      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 23, 16)            0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 368)               0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 4)                 1476      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45,772\n",
            "Trainable params: 45,622\n",
            "Non-trainable params: 150\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(X_spatial_prep, y_train_valid_prep, batch_size=64, epochs=100, shuffle=True,\n",
        "                    validation_split=0.2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vQNJ4-8OXRB0",
        "outputId": "e1032d4a-8ac2-49c5-8357-cedd35cd9490"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "106/106 [==============================] - 9s 45ms/step - loss: 0.2647 - acc: 0.9029 - val_loss: 0.0518 - val_acc: 0.9870\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2693 - acc: 0.8998 - val_loss: 0.0510 - val_acc: 0.9882\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2378 - acc: 0.9090 - val_loss: 0.0474 - val_acc: 0.9888\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2590 - acc: 0.9082 - val_loss: 0.0443 - val_acc: 0.9900\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2597 - acc: 0.9016 - val_loss: 0.0472 - val_acc: 0.9876\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2641 - acc: 0.9001 - val_loss: 0.0405 - val_acc: 0.9929\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2586 - acc: 0.9044 - val_loss: 0.0466 - val_acc: 0.9917\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2561 - acc: 0.9047 - val_loss: 0.0430 - val_acc: 0.9911\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2527 - acc: 0.9048 - val_loss: 0.0488 - val_acc: 0.9923\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2694 - acc: 0.9012 - val_loss: 0.0543 - val_acc: 0.9840\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2628 - acc: 0.9010 - val_loss: 0.0601 - val_acc: 0.9870\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2544 - acc: 0.9048 - val_loss: 0.0475 - val_acc: 0.9882\n",
            "Epoch 13/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2518 - acc: 0.9075 - val_loss: 0.0519 - val_acc: 0.9864\n",
            "Epoch 14/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2696 - acc: 0.8991 - val_loss: 0.0493 - val_acc: 0.9882\n",
            "Epoch 15/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2722 - acc: 0.8947 - val_loss: 0.0517 - val_acc: 0.9870\n",
            "Epoch 16/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2660 - acc: 0.8989 - val_loss: 0.0459 - val_acc: 0.9888\n",
            "Epoch 17/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2532 - acc: 0.9047 - val_loss: 0.0447 - val_acc: 0.9905\n",
            "Epoch 18/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2568 - acc: 0.9057 - val_loss: 0.0562 - val_acc: 0.9876\n",
            "Epoch 19/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2413 - acc: 0.9090 - val_loss: 0.0500 - val_acc: 0.9894\n",
            "Epoch 20/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2686 - acc: 0.8988 - val_loss: 0.0467 - val_acc: 0.9905\n",
            "Epoch 21/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2581 - acc: 0.9066 - val_loss: 0.0467 - val_acc: 0.9876\n",
            "Epoch 22/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2557 - acc: 0.9074 - val_loss: 0.0564 - val_acc: 0.9840\n",
            "Epoch 23/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2473 - acc: 0.9106 - val_loss: 0.0430 - val_acc: 0.9882\n",
            "Epoch 24/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2391 - acc: 0.9142 - val_loss: 0.0474 - val_acc: 0.9870\n",
            "Epoch 25/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2413 - acc: 0.9109 - val_loss: 0.0404 - val_acc: 0.9917\n",
            "Epoch 26/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2529 - acc: 0.9043 - val_loss: 0.0430 - val_acc: 0.9911\n",
            "Epoch 27/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2382 - acc: 0.9146 - val_loss: 0.0434 - val_acc: 0.9900\n",
            "Epoch 28/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2542 - acc: 0.9038 - val_loss: 0.0501 - val_acc: 0.9882\n",
            "Epoch 29/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2588 - acc: 0.9047 - val_loss: 0.0427 - val_acc: 0.9894\n",
            "Epoch 30/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2522 - acc: 0.9056 - val_loss: 0.0479 - val_acc: 0.9876\n",
            "Epoch 31/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2603 - acc: 0.9028 - val_loss: 0.0411 - val_acc: 0.9911\n",
            "Epoch 32/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2318 - acc: 0.9143 - val_loss: 0.0376 - val_acc: 0.9929\n",
            "Epoch 33/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2406 - acc: 0.9121 - val_loss: 0.0504 - val_acc: 0.9882\n",
            "Epoch 34/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2596 - acc: 0.9028 - val_loss: 0.0423 - val_acc: 0.9882\n",
            "Epoch 35/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2448 - acc: 0.9084 - val_loss: 0.0503 - val_acc: 0.9858\n",
            "Epoch 36/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2607 - acc: 0.9038 - val_loss: 0.0446 - val_acc: 0.9917\n",
            "Epoch 37/100\n",
            "106/106 [==============================] - 4s 37ms/step - loss: 0.2480 - acc: 0.9059 - val_loss: 0.0386 - val_acc: 0.9935\n",
            "Epoch 38/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2458 - acc: 0.9100 - val_loss: 0.0431 - val_acc: 0.9911\n",
            "Epoch 39/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2545 - acc: 0.9038 - val_loss: 0.0431 - val_acc: 0.9905\n",
            "Epoch 40/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2511 - acc: 0.9057 - val_loss: 0.0452 - val_acc: 0.9900\n",
            "Epoch 41/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2443 - acc: 0.9071 - val_loss: 0.0407 - val_acc: 0.9900\n",
            "Epoch 42/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2464 - acc: 0.9100 - val_loss: 0.0426 - val_acc: 0.9900\n",
            "Epoch 43/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2492 - acc: 0.9103 - val_loss: 0.0378 - val_acc: 0.9917\n",
            "Epoch 44/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2378 - acc: 0.9106 - val_loss: 0.0414 - val_acc: 0.9900\n",
            "Epoch 45/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2412 - acc: 0.9103 - val_loss: 0.0497 - val_acc: 0.9864\n",
            "Epoch 46/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2359 - acc: 0.9145 - val_loss: 0.0430 - val_acc: 0.9923\n",
            "Epoch 47/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2402 - acc: 0.9100 - val_loss: 0.0435 - val_acc: 0.9905\n",
            "Epoch 48/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2498 - acc: 0.9047 - val_loss: 0.0392 - val_acc: 0.9935\n",
            "Epoch 49/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2471 - acc: 0.9079 - val_loss: 0.0417 - val_acc: 0.9911\n",
            "Epoch 50/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2513 - acc: 0.9065 - val_loss: 0.0427 - val_acc: 0.9905\n",
            "Epoch 51/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2251 - acc: 0.9193 - val_loss: 0.0403 - val_acc: 0.9923\n",
            "Epoch 52/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2330 - acc: 0.9158 - val_loss: 0.0347 - val_acc: 0.9929\n",
            "Epoch 53/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2398 - acc: 0.9121 - val_loss: 0.0473 - val_acc: 0.9876\n",
            "Epoch 54/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2230 - acc: 0.9131 - val_loss: 0.0386 - val_acc: 0.9923\n",
            "Epoch 55/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2407 - acc: 0.9108 - val_loss: 0.0471 - val_acc: 0.9894\n",
            "Epoch 56/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2510 - acc: 0.9048 - val_loss: 0.0394 - val_acc: 0.9917\n",
            "Epoch 57/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2350 - acc: 0.9116 - val_loss: 0.0403 - val_acc: 0.9900\n",
            "Epoch 58/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2521 - acc: 0.9081 - val_loss: 0.0398 - val_acc: 0.9911\n",
            "Epoch 59/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2273 - acc: 0.9137 - val_loss: 0.0358 - val_acc: 0.9947\n",
            "Epoch 60/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2510 - acc: 0.9057 - val_loss: 0.0373 - val_acc: 0.9929\n",
            "Epoch 61/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2337 - acc: 0.9150 - val_loss: 0.0378 - val_acc: 0.9935\n",
            "Epoch 62/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2445 - acc: 0.9085 - val_loss: 0.0383 - val_acc: 0.9894\n",
            "Epoch 63/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2403 - acc: 0.9099 - val_loss: 0.0356 - val_acc: 0.9947\n",
            "Epoch 64/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2412 - acc: 0.9112 - val_loss: 0.0385 - val_acc: 0.9911\n",
            "Epoch 65/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2339 - acc: 0.9084 - val_loss: 0.0403 - val_acc: 0.9894\n",
            "Epoch 66/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2423 - acc: 0.9125 - val_loss: 0.0433 - val_acc: 0.9876\n",
            "Epoch 67/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2324 - acc: 0.9142 - val_loss: 0.0343 - val_acc: 0.9929\n",
            "Epoch 68/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2551 - acc: 0.9056 - val_loss: 0.0466 - val_acc: 0.9905\n",
            "Epoch 69/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2287 - acc: 0.9137 - val_loss: 0.0443 - val_acc: 0.9894\n",
            "Epoch 70/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2426 - acc: 0.9116 - val_loss: 0.0399 - val_acc: 0.9905\n",
            "Epoch 71/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2374 - acc: 0.9146 - val_loss: 0.0329 - val_acc: 0.9935\n",
            "Epoch 72/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2449 - acc: 0.9082 - val_loss: 0.0484 - val_acc: 0.9864\n",
            "Epoch 73/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2274 - acc: 0.9118 - val_loss: 0.0415 - val_acc: 0.9929\n",
            "Epoch 74/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2411 - acc: 0.9127 - val_loss: 0.0346 - val_acc: 0.9935\n",
            "Epoch 75/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2265 - acc: 0.9150 - val_loss: 0.0445 - val_acc: 0.9876\n",
            "Epoch 76/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2353 - acc: 0.9140 - val_loss: 0.0380 - val_acc: 0.9894\n",
            "Epoch 77/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2508 - acc: 0.9084 - val_loss: 0.0442 - val_acc: 0.9900\n",
            "Epoch 78/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2403 - acc: 0.9053 - val_loss: 0.0384 - val_acc: 0.9935\n",
            "Epoch 79/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2458 - acc: 0.9130 - val_loss: 0.0417 - val_acc: 0.9911\n",
            "Epoch 80/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2442 - acc: 0.9088 - val_loss: 0.0411 - val_acc: 0.9900\n",
            "Epoch 81/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2401 - acc: 0.9119 - val_loss: 0.0387 - val_acc: 0.9935\n",
            "Epoch 82/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2486 - acc: 0.9065 - val_loss: 0.0373 - val_acc: 0.9917\n",
            "Epoch 83/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2159 - acc: 0.9215 - val_loss: 0.0341 - val_acc: 0.9947\n",
            "Epoch 84/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2235 - acc: 0.9192 - val_loss: 0.0350 - val_acc: 0.9923\n",
            "Epoch 85/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2307 - acc: 0.9147 - val_loss: 0.0410 - val_acc: 0.9900\n",
            "Epoch 86/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2443 - acc: 0.9040 - val_loss: 0.0384 - val_acc: 0.9941\n",
            "Epoch 87/100\n",
            "106/106 [==============================] - 4s 38ms/step - loss: 0.2343 - acc: 0.9130 - val_loss: 0.0400 - val_acc: 0.9905\n",
            "Epoch 88/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2327 - acc: 0.9168 - val_loss: 0.0370 - val_acc: 0.9917\n",
            "Epoch 89/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2492 - acc: 0.9093 - val_loss: 0.0392 - val_acc: 0.9923\n",
            "Epoch 90/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2223 - acc: 0.9173 - val_loss: 0.0357 - val_acc: 0.9923\n",
            "Epoch 91/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2164 - acc: 0.9241 - val_loss: 0.0368 - val_acc: 0.9894\n",
            "Epoch 92/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2455 - acc: 0.9047 - val_loss: 0.0397 - val_acc: 0.9888\n",
            "Epoch 93/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2420 - acc: 0.9113 - val_loss: 0.0455 - val_acc: 0.9905\n",
            "Epoch 94/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2357 - acc: 0.9125 - val_loss: 0.0442 - val_acc: 0.9870\n",
            "Epoch 95/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2282 - acc: 0.9199 - val_loss: 0.0380 - val_acc: 0.9947\n",
            "Epoch 96/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2397 - acc: 0.9100 - val_loss: 0.0354 - val_acc: 0.9923\n",
            "Epoch 97/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2287 - acc: 0.9137 - val_loss: 0.0371 - val_acc: 0.9935\n",
            "Epoch 98/100\n",
            "106/106 [==============================] - 4s 34ms/step - loss: 0.2337 - acc: 0.9094 - val_loss: 0.0346 - val_acc: 0.9929\n",
            "Epoch 99/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2110 - acc: 0.9211 - val_loss: 0.0318 - val_acc: 0.9917\n",
            "Epoch 100/100\n",
            "106/106 [==============================] - 4s 35ms/step - loss: 0.2491 - acc: 0.9081 - val_loss: 0.0347 - val_acc: 0.9941\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_spatial_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5W7-Qi6vG_9",
        "outputId": "079f1abc-2a84-4051-8e7a-964f2166494f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 13ms/step - loss: 0.9236 - acc: 0.7698\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.923640251159668, 0.7697516679763794]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'], label='train')\n",
        "plt.plot(history.history['val_acc'], label='test')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qS3hSLpYWYAa",
        "outputId": "b772f0fd-5a08-4ac8-90eb-d8b6af1f9e57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}