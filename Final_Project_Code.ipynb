{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Final Project Code.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreyasrajesh0308/NNDL-proj/blob/main/Final_Project_Code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import Necessary Dependencies"
      ],
      "metadata": {
        "id": "p19wBIfg-VYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from matplotlib import pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.layers import Conv3D, MaxPooling3D, Conv2D, BatchNormalization, Activation, Flatten, Dense, Dropout, LSTM, Input, TimeDistributed, Permute, Reshape, MaxPooling2D, GRU\n",
        "from keras import initializers, Model, optimizers, callbacks\n",
        "from keras import Sequential\n",
        "from keras import backend as K\n",
        "from keras.models import load_model\n",
        "from keras.callbacks import Callback\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score"
      ],
      "metadata": {
        "id": "2gXQ7pXTIqqT"
      },
      "execution_count": 191,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data \n",
        "\n",
        "Load preprocessed data from Google Drive"
      ],
      "metadata": {
        "id": "5Fl4uJtrAqU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Scu17GL9A7i5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e86e2fdb-edf5-479e-d159-38472d1a6fbc"
      },
      "execution_count": 192,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = np.load(\"/content/drive/MyDrive/eeg_project/X_test.npy\")\n",
        "y_test = np.load(\"/content/drive/MyDrive/eeg_project/y_test.npy\")\n",
        "person_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/person_train_valid.npy\")\n",
        "X_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/X_train_valid.npy\")\n",
        "y_train_valid = np.load(\"/content/drive/MyDrive/eeg_project/y_train_valid.npy\")\n",
        "person_test = np.load(\"/content/drive/MyDrive/eeg_project/person_test.npy\")"
      ],
      "metadata": {
        "id": "foTd2q3JIreb"
      },
      "execution_count": 193,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observing some basic characteristics about the data, like shape and output classes"
      ],
      "metadata": {
        "id": "y_akRtg--v6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n"
      ],
      "metadata": {
        "id": "g6VAQSVEUtIW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "119dcbb8-7646-4410-8b35-b8f1e5019c92"
      },
      "execution_count": 194,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.unique(y_train_valid))\n",
        "print(np.unique(y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bhxR60r5IzuR",
        "outputId": "93c2c4d0-f80b-4747-92a1-cab48817e42e"
      },
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[769 770 771 772]\n",
            "[769 770 771 772]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess test data to convert it to classes in range 0-3"
      ],
      "metadata": {
        "id": "OWyid62p-5ml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 4\n",
        "y_train_valid = y_train_valid-769\n",
        "y_test = y_test-769"
      ],
      "metadata": {
        "id": "U4K1wzmfI4Rb"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split Data into train and Valid"
      ],
      "metadata": {
        "id": "L4tuh59BQ-FV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(X_train_valid, y_train_valid, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6xo7RhdpRBPx"
      },
      "execution_count": 197,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print ('Training data shape: {}'.format(X_train.shape))\n",
        "print ('Valid data shape: {}'.format(X_valid.shape))\n",
        "print ('Training target shape: {}'.format(y_train.shape))\n",
        "print ('Valid target shape: {}'.format(y_valid.shape))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "McUbRkeARWXE",
        "outputId": "658f8205-d9b1-4a85-ede1-d6c3397b9175"
      },
      "execution_count": 198,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data shape: (1692, 22, 1000)\n",
            "Valid data shape: (423, 22, 1000)\n",
            "Training target shape: (1692,)\n",
            "Valid target shape: (423,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Data Preprocessing and Augmentation\n",
        "Here we perform data preprocessing by first trimming the data down and reducing the number of time stamps through a combination of epoch extraction (discarding last 500 samples) and max pool to reduce it from 1000 to 250 time stamps.\n",
        "We also subsampling of the data, and add in some noise to enhance the size of the training dataset"
      ],
      "metadata": {
        "id": "5sZF8fDf_CzI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep(X,y,sub_sample,average,noise):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    \n",
        "    # Averaging + noise \n",
        "    X_average = np.mean(X.reshape(X.shape[0], X.shape[1], -1, average),axis=3)\n",
        "    X_average = X_average + np.random.normal(0.0, 0.5, X_average.shape)\n",
        "    \n",
        "    total_X = np.vstack((total_X, X_average))\n",
        "    total_y = np.hstack((total_y, y))\n",
        "    print('Shape of X after averaging+noise and concatenating:',total_X.shape)\n",
        "    \n",
        "    # Subsampling\n",
        "    \n",
        "    for i in range(sub_sample):\n",
        "        \n",
        "        X_subsample = X[:, :, i::sub_sample] + \\\n",
        "                            (np.random.normal(0.0, 0.5, X[:, :,i::sub_sample].shape) if noise else 0.0)\n",
        "            \n",
        "        total_X = np.vstack((total_X, X_subsample))\n",
        "        total_y = np.hstack((total_y, y))\n",
        "        \n",
        "    \n",
        "    print('Shape of X after subsampling and concatenating:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_train_prep,y_train_prep= data_prep(X_train,y_train,2,2,True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "0-ubjiWDpR_W",
        "outputId": "d42b50d8-1a72-441b-8ebf-03e614575735"
      },
      "execution_count": 199,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (1692, 22, 500)\n",
            "Shape of X after maxpooling: (1692, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (3384, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (6768, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep_test(X,y, sub_sample=2):\n",
        "    \n",
        "    total_X = None\n",
        "    total_y = None\n",
        "    \n",
        "    # Trimming the data (sample,22,1000) -> (sample,22,500)\n",
        "    X = X[:,:,0:500]\n",
        "    print('Shape of X after trimming:',X.shape)\n",
        "    \n",
        "    # Maxpooling the data (sample,22,1000) -> (sample,22,500/sub_sample)\n",
        "    X_max = np.max(X.reshape(X.shape[0], X.shape[1], -1, sub_sample), axis=3)\n",
        "    \n",
        "    \n",
        "    total_X = X_max\n",
        "    total_y = y\n",
        "    print('Shape of X after maxpooling:',total_X.shape)\n",
        "    return total_X,total_y\n",
        "\n",
        "\n",
        "X_test,y_test = data_prep_test(X_test, y_test,2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IhtalewhtI6R",
        "outputId": "c7d46255-78df-441c-99f1-4f180d03ffc5"
      },
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (443, 22, 500)\n",
            "Shape of X after maxpooling: (443, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_valid_prep,y_valid_prep = data_prep_test(X_valid, y_valid,2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tu4DoAKFRsip",
        "outputId": "e3b413b4-8d28-45aa-92a6-cc0667496662"
      },
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (423, 22, 500)\n",
            "Shape of X after maxpooling: (423, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Creating Spatial Mesh\n",
        "We now convert the 2D set of time stamps (22x250) to a 3D 6x7 mesh (6x7x250) to model the relative proximity of the location of the electrodes on the scalp\n"
      ],
      "metadata": {
        "id": "UQd2xrA-_t_L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def spatial_prep(X):\n",
        "  X = np.swapaxes(X, 0, 1)\n",
        "\n",
        "  sh = (X.shape[1],X.shape[2])\n",
        "  new_X = np.array([[np.zeros(sh), np.zeros(sh), np.zeros(sh), X[0,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), X[1,:,:], X[2,:,:], X[3,:,:], X[4,:,:], X[5,:,:], np.zeros(sh)],\n",
        "                [X[6,:,:],X[7,:,:],X[8,:,:],X[9,:,:],X[10,:,:],X[11,:,:],X[12,:,:]],\n",
        "                [np.zeros(sh), X[13,:,:],X[14,:,:],X[15,:,:],X[16,:,:],X[17,:,:], np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), X[18,:,:],X[19,:,:],X[20,:,:], np.zeros(sh), np.zeros(sh)],\n",
        "                [np.zeros(sh), np.zeros(sh), np.zeros(sh), X[21,:,:], np.zeros(sh), np.zeros(sh), np.zeros(sh)]])\n",
        "\n",
        "  \n",
        "  new_X = np.swapaxes(new_X, 0,2)\n",
        "  new_X = np.swapaxes(new_X, 1,2)\n",
        "  return new_X"
      ],
      "metadata": {
        "id": "-u8tro1jwuon"
      },
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_train = spatial_prep(X_train_prep)"
      ],
      "metadata": {
        "id": "flRuB5gPygqF"
      },
      "execution_count": 203,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_val = spatial_prep(X_valid_prep)"
      ],
      "metadata": {
        "id": "fLMcU9RzwFT-"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_test = spatial_prep(X_test)"
      ],
      "metadata": {
        "id": "spWmKQWT3qd1"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_spatial_train.shape, X_spatial_val.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "l3rD3lYUyumZ",
        "outputId": "876d92dc-5cf5-4443-d712-7bfd76eab5d9"
      },
      "execution_count": 206,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6768, 6, 7, 250) (423, 6, 7, 250) (443, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now convert the labels to one hot encoded vectors"
      ],
      "metadata": {
        "id": "0Qu-37c5AIaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train_prep = keras.utils.to_categorical(y_train_prep, num_classes)\n",
        "y_valid_prep = keras.utils.to_categorical(y_valid_prep, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "metadata": {
        "id": "NZWq7ZCFq5z9"
      },
      "execution_count": 207,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_prep.shape,y_train_prep.shape, X_spatial_train.shape)\n",
        "print(X_valid_prep.shape,y_valid_prep.shape, X_spatial_val.shape)\n",
        "print(X_test.shape, y_test.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "YFZfw6CuqeBs",
        "outputId": "271490ab-8e62-4a42-c6ea-3c144568d52c"
      },
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6768, 22, 250) (6768, 4) (6768, 6, 7, 250)\n",
            "(423, 22, 250) (423, 4) (423, 6, 7, 250)\n",
            "(443, 22, 250) (443, 4) (443, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To streamline the input into the model, we also swap the axis of the input data to make the 3D convolution of the mesh easier"
      ],
      "metadata": {
        "id": "skmL_YwRAYjE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_train = np.swapaxes(X_spatial_train , 1,3)\n",
        "X_spatial_train = np.swapaxes(X_spatial_train , 2,3)\n",
        "X_spatial_val = np.swapaxes(X_spatial_val , 1,3)\n",
        "X_spatial_val = np.swapaxes(X_spatial_val , 2,3)\n",
        "X_spatial_test  = np.swapaxes(X_spatial_test , 1,3)\n",
        "X_spatial_test = np.swapaxes(X_spatial_test , 2,3)"
      ],
      "metadata": {
        "id": "b3_pN5eAY1bg"
      },
      "execution_count": 209,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_prep.shape,y_train_prep.shape, X_spatial_train.shape)\n",
        "print(X_valid_prep.shape,y_valid_prep.shape, X_spatial_val.shape)\n",
        "print(X_test.shape, y_test.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1MC6IQn7YEF-",
        "outputId": "3085b6cb-207d-4ea3-b357-464d6eb29a05"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(6768, 22, 250) (6768, 4) (6768, 250, 6, 7)\n",
            "(423, 22, 250) (423, 4) (423, 250, 6, 7)\n",
            "(443, 22, 250) (443, 4) (443, 250, 6, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3D conv + GRU with all subjects\n",
        "\n",
        "We now define the model architecture to be used for the task, and subsequently train it and test its accuracy. We also use this section to perform hyperparamater tuning and architecture selection to arrive that the model that performs best on this task."
      ],
      "metadata": {
        "id": "IxgOEM-3WIPG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lecun = initializers.lecun_normal(seed=42)\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Reshape((250, 6, 7, 1), input_shape = (250,6,7)))\n",
        "\n",
        "model.add(Conv3D(filters=25, kernel_size=(10,1,1), kernel_initializer = lecun, strides=1, data_format=\"channels_last\"))\n",
        "model.add(Conv3D(filters=25, kernel_size=(1,6,7), kernel_initializer = lecun ))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling3D(pool_size = (3,1,1), strides = (3,1,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Conv Pool Block 2\n",
        "model.add(Conv3D(filters = 50, kernel_size = (10,1,1), activation = 'elu', kernel_initializer = lecun))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Activation(activation = 'elu'))\n",
        "model.add(MaxPooling3D(pool_size = (3,1,1), strides = (3,1,1)))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "model.add(Permute((1, 4, 3, 2)))\n",
        "model.add(Reshape((23, 50)))\n",
        "\n",
        "# GRU layers\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "model.add(GRU(16, return_sequences=True))\n",
        "model.add(Dropout(0.5))\n",
        "\n",
        "# Dense layers\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=4, activation='softmax'))\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "print(model.summary())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "BSm44W_mWdxt",
        "outputId": "a3036876-9b6c-4b70-ca90-6eb6a323008a"
      },
      "execution_count": 264,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " reshape_22 (Reshape)        (None, 250, 6, 7, 1)      0         \n",
            "                                                                 \n",
            " conv3d_33 (Conv3D)          (None, 241, 6, 7, 25)     275       \n",
            "                                                                 \n",
            " conv3d_34 (Conv3D)          (None, 241, 1, 1, 25)     26275     \n",
            "                                                                 \n",
            " batch_normalization_22 (Bat  (None, 241, 1, 1, 25)    100       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_22 (Activation)  (None, 241, 1, 1, 25)     0         \n",
            "                                                                 \n",
            " max_pooling3d_22 (MaxPoolin  (None, 80, 1, 1, 25)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 80, 1, 1, 25)      0         \n",
            "                                                                 \n",
            " conv3d_35 (Conv3D)          (None, 71, 1, 1, 50)      12550     \n",
            "                                                                 \n",
            " batch_normalization_23 (Bat  (None, 71, 1, 1, 50)     200       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " activation_23 (Activation)  (None, 71, 1, 1, 50)      0         \n",
            "                                                                 \n",
            " max_pooling3d_23 (MaxPoolin  (None, 23, 1, 1, 50)     0         \n",
            " g3D)                                                            \n",
            "                                                                 \n",
            " dropout_34 (Dropout)        (None, 23, 1, 1, 50)      0         \n",
            "                                                                 \n",
            " permute_11 (Permute)        (None, 23, 50, 1, 1)      0         \n",
            "                                                                 \n",
            " reshape_23 (Reshape)        (None, 23, 50)            0         \n",
            "                                                                 \n",
            " gru_22 (GRU)                (None, 23, 16)            3264      \n",
            "                                                                 \n",
            " gru_23 (GRU)                (None, 23, 16)            1632      \n",
            "                                                                 \n",
            " dropout_35 (Dropout)        (None, 23, 16)            0         \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 368)               0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 4)                 1476      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 45,772\n",
            "Trainable params: 45,622\n",
            "Non-trainable params: 150\n",
            "_________________________________________________________________\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n",
        "history = model.fit(X_spatial_train, y_train_prep, batch_size=64, epochs=200,\n",
        "                    validation_data=(X_spatial_val, y_valid_prep))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vQNJ4-8OXRB0",
        "outputId": "7b2f2431-b012-4a5b-8908-7c9dd5eb7bea"
      },
      "execution_count": 212,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "106/106 [==============================] - 109s 26ms/step - loss: 1.4067 - acc: 0.2881 - val_loss: 1.3143 - val_acc: 0.3664\n",
            "Epoch 2/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.2927 - acc: 0.3907 - val_loss: 1.2531 - val_acc: 0.4161\n",
            "Epoch 3/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.2260 - acc: 0.4350 - val_loss: 1.2264 - val_acc: 0.4468\n",
            "Epoch 4/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.1860 - acc: 0.4638 - val_loss: 1.1838 - val_acc: 0.4610\n",
            "Epoch 5/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.1305 - acc: 0.5163 - val_loss: 1.1844 - val_acc: 0.4941\n",
            "Epoch 6/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.0809 - acc: 0.5312 - val_loss: 1.0873 - val_acc: 0.5437\n",
            "Epoch 7/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 1.0366 - acc: 0.5669 - val_loss: 1.0691 - val_acc: 0.5366\n",
            "Epoch 8/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.9885 - acc: 0.5959 - val_loss: 1.0509 - val_acc: 0.5674\n",
            "Epoch 9/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.9719 - acc: 0.6015 - val_loss: 1.0324 - val_acc: 0.5792\n",
            "Epoch 10/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.9439 - acc: 0.6139 - val_loss: 1.0206 - val_acc: 0.5887\n",
            "Epoch 11/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.9181 - acc: 0.6281 - val_loss: 0.9859 - val_acc: 0.5910\n",
            "Epoch 12/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.9112 - acc: 0.6287 - val_loss: 0.9564 - val_acc: 0.6099\n",
            "Epoch 13/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.8824 - acc: 0.6469 - val_loss: 0.9582 - val_acc: 0.6076\n",
            "Epoch 14/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.8635 - acc: 0.6525 - val_loss: 0.9414 - val_acc: 0.6265\n",
            "Epoch 15/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.8494 - acc: 0.6616 - val_loss: 0.9806 - val_acc: 0.6076\n",
            "Epoch 16/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.8271 - acc: 0.6761 - val_loss: 0.9307 - val_acc: 0.6241\n",
            "Epoch 17/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.8140 - acc: 0.6748 - val_loss: 1.0134 - val_acc: 0.6170\n",
            "Epoch 18/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7832 - acc: 0.6956 - val_loss: 0.8662 - val_acc: 0.6501\n",
            "Epoch 19/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7911 - acc: 0.6868 - val_loss: 0.8876 - val_acc: 0.6501\n",
            "Epoch 20/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7600 - acc: 0.6999 - val_loss: 0.9028 - val_acc: 0.6525\n",
            "Epoch 21/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7348 - acc: 0.7058 - val_loss: 0.8593 - val_acc: 0.6714\n",
            "Epoch 22/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7236 - acc: 0.7162 - val_loss: 0.8587 - val_acc: 0.6619\n",
            "Epoch 23/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.7209 - acc: 0.7156 - val_loss: 0.8853 - val_acc: 0.6501\n",
            "Epoch 24/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6950 - acc: 0.7277 - val_loss: 0.8439 - val_acc: 0.6738\n",
            "Epoch 25/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6890 - acc: 0.7299 - val_loss: 0.8753 - val_acc: 0.6572\n",
            "Epoch 26/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6754 - acc: 0.7364 - val_loss: 0.8755 - val_acc: 0.6596\n",
            "Epoch 27/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6718 - acc: 0.7401 - val_loss: 0.8408 - val_acc: 0.6738\n",
            "Epoch 28/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6665 - acc: 0.7395 - val_loss: 0.8139 - val_acc: 0.7045\n",
            "Epoch 29/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6460 - acc: 0.7530 - val_loss: 0.8306 - val_acc: 0.6809\n",
            "Epoch 30/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6454 - acc: 0.7510 - val_loss: 0.8327 - val_acc: 0.6856\n",
            "Epoch 31/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6320 - acc: 0.7572 - val_loss: 0.8394 - val_acc: 0.6856\n",
            "Epoch 32/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6261 - acc: 0.7586 - val_loss: 0.8069 - val_acc: 0.6879\n",
            "Epoch 33/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6174 - acc: 0.7621 - val_loss: 0.8576 - val_acc: 0.6761\n",
            "Epoch 34/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6077 - acc: 0.7630 - val_loss: 0.8438 - val_acc: 0.7069\n",
            "Epoch 35/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6050 - acc: 0.7646 - val_loss: 0.8845 - val_acc: 0.6690\n",
            "Epoch 36/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.6039 - acc: 0.7683 - val_loss: 0.8360 - val_acc: 0.6832\n",
            "Epoch 37/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5812 - acc: 0.7757 - val_loss: 0.8683 - val_acc: 0.6879\n",
            "Epoch 38/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5791 - acc: 0.7803 - val_loss: 0.8969 - val_acc: 0.6525\n",
            "Epoch 39/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5828 - acc: 0.7750 - val_loss: 0.8421 - val_acc: 0.6809\n",
            "Epoch 40/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5587 - acc: 0.7855 - val_loss: 0.8240 - val_acc: 0.6927\n",
            "Epoch 41/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5685 - acc: 0.7877 - val_loss: 0.8051 - val_acc: 0.6998\n",
            "Epoch 42/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5543 - acc: 0.7853 - val_loss: 0.9168 - val_acc: 0.6619\n",
            "Epoch 43/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5569 - acc: 0.7844 - val_loss: 0.8706 - val_acc: 0.6785\n",
            "Epoch 44/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5594 - acc: 0.7840 - val_loss: 0.8214 - val_acc: 0.7045\n",
            "Epoch 45/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5367 - acc: 0.7940 - val_loss: 0.8973 - val_acc: 0.6572\n",
            "Epoch 46/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5281 - acc: 0.8007 - val_loss: 0.8510 - val_acc: 0.6761\n",
            "Epoch 47/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5270 - acc: 0.7934 - val_loss: 0.7848 - val_acc: 0.6974\n",
            "Epoch 48/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5147 - acc: 0.8045 - val_loss: 0.8131 - val_acc: 0.7069\n",
            "Epoch 49/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5154 - acc: 0.8022 - val_loss: 0.8434 - val_acc: 0.6643\n",
            "Epoch 50/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.5061 - acc: 0.8097 - val_loss: 0.8365 - val_acc: 0.6974\n",
            "Epoch 51/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4978 - acc: 0.8076 - val_loss: 0.8364 - val_acc: 0.6903\n",
            "Epoch 52/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4973 - acc: 0.8076 - val_loss: 0.8068 - val_acc: 0.6903\n",
            "Epoch 53/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4963 - acc: 0.8059 - val_loss: 0.8457 - val_acc: 0.6785\n",
            "Epoch 54/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4850 - acc: 0.8180 - val_loss: 0.9106 - val_acc: 0.6761\n",
            "Epoch 55/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4804 - acc: 0.8118 - val_loss: 0.8266 - val_acc: 0.7021\n",
            "Epoch 56/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4726 - acc: 0.8178 - val_loss: 0.8337 - val_acc: 0.6974\n",
            "Epoch 57/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4699 - acc: 0.8205 - val_loss: 0.8682 - val_acc: 0.6832\n",
            "Epoch 58/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4702 - acc: 0.8257 - val_loss: 0.8747 - val_acc: 0.6903\n",
            "Epoch 59/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4681 - acc: 0.8187 - val_loss: 0.8801 - val_acc: 0.6832\n",
            "Epoch 60/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.4703 - acc: 0.8215 - val_loss: 0.8702 - val_acc: 0.6738\n",
            "Epoch 61/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4636 - acc: 0.8243 - val_loss: 0.8522 - val_acc: 0.6761\n",
            "Epoch 62/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.4538 - acc: 0.8248 - val_loss: 0.8093 - val_acc: 0.6998\n",
            "Epoch 63/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4501 - acc: 0.8323 - val_loss: 0.8177 - val_acc: 0.7092\n",
            "Epoch 64/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4498 - acc: 0.8296 - val_loss: 0.8859 - val_acc: 0.6596\n",
            "Epoch 65/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.4483 - acc: 0.8324 - val_loss: 0.8373 - val_acc: 0.7045\n",
            "Epoch 66/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4595 - acc: 0.8249 - val_loss: 0.8078 - val_acc: 0.7258\n",
            "Epoch 67/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4327 - acc: 0.8350 - val_loss: 0.8256 - val_acc: 0.7163\n",
            "Epoch 68/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.4323 - acc: 0.8332 - val_loss: 0.8305 - val_acc: 0.7092\n",
            "Epoch 69/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4272 - acc: 0.8395 - val_loss: 0.8456 - val_acc: 0.7069\n",
            "Epoch 70/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4291 - acc: 0.8413 - val_loss: 0.9597 - val_acc: 0.6619\n",
            "Epoch 71/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4245 - acc: 0.8395 - val_loss: 0.8532 - val_acc: 0.6927\n",
            "Epoch 72/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4235 - acc: 0.8387 - val_loss: 0.9013 - val_acc: 0.6832\n",
            "Epoch 73/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4371 - acc: 0.8289 - val_loss: 0.9320 - val_acc: 0.6643\n",
            "Epoch 74/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4197 - acc: 0.8409 - val_loss: 0.8781 - val_acc: 0.6856\n",
            "Epoch 75/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4098 - acc: 0.8426 - val_loss: 0.8743 - val_acc: 0.6785\n",
            "Epoch 76/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3992 - acc: 0.8500 - val_loss: 0.8777 - val_acc: 0.6785\n",
            "Epoch 77/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4192 - acc: 0.8378 - val_loss: 0.8593 - val_acc: 0.7021\n",
            "Epoch 78/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4092 - acc: 0.8465 - val_loss: 0.8469 - val_acc: 0.7045\n",
            "Epoch 79/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3967 - acc: 0.8521 - val_loss: 0.9529 - val_acc: 0.6856\n",
            "Epoch 80/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4010 - acc: 0.8493 - val_loss: 0.9543 - val_acc: 0.6785\n",
            "Epoch 81/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.4045 - acc: 0.8454 - val_loss: 0.9447 - val_acc: 0.6785\n",
            "Epoch 82/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3926 - acc: 0.8521 - val_loss: 0.9087 - val_acc: 0.6667\n",
            "Epoch 83/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4041 - acc: 0.8437 - val_loss: 0.9128 - val_acc: 0.6856\n",
            "Epoch 84/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3731 - acc: 0.8546 - val_loss: 0.9511 - val_acc: 0.6619\n",
            "Epoch 85/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3975 - acc: 0.8511 - val_loss: 0.9232 - val_acc: 0.6785\n",
            "Epoch 86/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3865 - acc: 0.8534 - val_loss: 0.9198 - val_acc: 0.6809\n",
            "Epoch 87/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3868 - acc: 0.8518 - val_loss: 0.9279 - val_acc: 0.6927\n",
            "Epoch 88/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.4094 - acc: 0.8447 - val_loss: 0.9397 - val_acc: 0.6809\n",
            "Epoch 89/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3803 - acc: 0.8561 - val_loss: 0.8806 - val_acc: 0.6927\n",
            "Epoch 90/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3741 - acc: 0.8589 - val_loss: 0.8488 - val_acc: 0.6974\n",
            "Epoch 91/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3857 - acc: 0.8536 - val_loss: 0.8548 - val_acc: 0.6903\n",
            "Epoch 92/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3775 - acc: 0.8552 - val_loss: 0.9121 - val_acc: 0.6879\n",
            "Epoch 93/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3853 - acc: 0.8536 - val_loss: 0.8939 - val_acc: 0.7045\n",
            "Epoch 94/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3804 - acc: 0.8586 - val_loss: 0.9405 - val_acc: 0.6809\n",
            "Epoch 95/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3726 - acc: 0.8611 - val_loss: 0.8871 - val_acc: 0.6856\n",
            "Epoch 96/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3638 - acc: 0.8630 - val_loss: 0.9509 - val_acc: 0.6974\n",
            "Epoch 97/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.3687 - acc: 0.8604 - val_loss: 0.9922 - val_acc: 0.6761\n",
            "Epoch 98/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3733 - acc: 0.8582 - val_loss: 0.9214 - val_acc: 0.6761\n",
            "Epoch 99/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3684 - acc: 0.8605 - val_loss: 0.9724 - val_acc: 0.6714\n",
            "Epoch 100/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3639 - acc: 0.8599 - val_loss: 0.9083 - val_acc: 0.6879\n",
            "Epoch 101/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3498 - acc: 0.8697 - val_loss: 0.9515 - val_acc: 0.6809\n",
            "Epoch 102/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3755 - acc: 0.8559 - val_loss: 0.9426 - val_acc: 0.6761\n",
            "Epoch 103/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3488 - acc: 0.8700 - val_loss: 0.9417 - val_acc: 0.6879\n",
            "Epoch 104/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3431 - acc: 0.8703 - val_loss: 0.9584 - val_acc: 0.6903\n",
            "Epoch 105/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3567 - acc: 0.8601 - val_loss: 0.9331 - val_acc: 0.6903\n",
            "Epoch 106/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3608 - acc: 0.8611 - val_loss: 0.9723 - val_acc: 0.6761\n",
            "Epoch 107/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3665 - acc: 0.8645 - val_loss: 0.9179 - val_acc: 0.6950\n",
            "Epoch 108/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3486 - acc: 0.8706 - val_loss: 0.9209 - val_acc: 0.7045\n",
            "Epoch 109/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3607 - acc: 0.8647 - val_loss: 0.9004 - val_acc: 0.6927\n",
            "Epoch 110/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3584 - acc: 0.8667 - val_loss: 0.9214 - val_acc: 0.6998\n",
            "Epoch 111/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3605 - acc: 0.8632 - val_loss: 0.9494 - val_acc: 0.6785\n",
            "Epoch 112/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3575 - acc: 0.8629 - val_loss: 0.9175 - val_acc: 0.7092\n",
            "Epoch 113/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3501 - acc: 0.8667 - val_loss: 0.9285 - val_acc: 0.6761\n",
            "Epoch 114/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3447 - acc: 0.8667 - val_loss: 0.8547 - val_acc: 0.7234\n",
            "Epoch 115/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3439 - acc: 0.8685 - val_loss: 0.8962 - val_acc: 0.6879\n",
            "Epoch 116/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3510 - acc: 0.8695 - val_loss: 0.8952 - val_acc: 0.6856\n",
            "Epoch 117/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3438 - acc: 0.8664 - val_loss: 0.9488 - val_acc: 0.6998\n",
            "Epoch 118/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3130 - acc: 0.8805 - val_loss: 0.9318 - val_acc: 0.6927\n",
            "Epoch 119/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3199 - acc: 0.8814 - val_loss: 0.9629 - val_acc: 0.7092\n",
            "Epoch 120/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3340 - acc: 0.8759 - val_loss: 0.9996 - val_acc: 0.6809\n",
            "Epoch 121/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.3308 - acc: 0.8726 - val_loss: 0.9663 - val_acc: 0.6950\n",
            "Epoch 122/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3382 - acc: 0.8734 - val_loss: 0.9407 - val_acc: 0.7092\n",
            "Epoch 123/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3235 - acc: 0.8788 - val_loss: 0.8955 - val_acc: 0.7021\n",
            "Epoch 124/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3370 - acc: 0.8679 - val_loss: 0.9148 - val_acc: 0.6903\n",
            "Epoch 125/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3231 - acc: 0.8762 - val_loss: 0.9265 - val_acc: 0.6690\n",
            "Epoch 126/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3288 - acc: 0.8777 - val_loss: 0.9310 - val_acc: 0.6856\n",
            "Epoch 127/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3166 - acc: 0.8821 - val_loss: 0.9660 - val_acc: 0.7092\n",
            "Epoch 128/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3214 - acc: 0.8783 - val_loss: 0.9189 - val_acc: 0.6974\n",
            "Epoch 129/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3250 - acc: 0.8784 - val_loss: 1.0006 - val_acc: 0.6856\n",
            "Epoch 130/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3407 - acc: 0.8751 - val_loss: 0.9313 - val_acc: 0.7210\n",
            "Epoch 131/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3380 - acc: 0.8700 - val_loss: 0.9202 - val_acc: 0.7069\n",
            "Epoch 132/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3414 - acc: 0.8704 - val_loss: 0.9447 - val_acc: 0.6903\n",
            "Epoch 133/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3305 - acc: 0.8744 - val_loss: 0.9455 - val_acc: 0.6998\n",
            "Epoch 134/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3287 - acc: 0.8726 - val_loss: 0.9586 - val_acc: 0.6785\n",
            "Epoch 135/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3241 - acc: 0.8753 - val_loss: 0.9649 - val_acc: 0.6643\n",
            "Epoch 136/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3240 - acc: 0.8818 - val_loss: 0.9699 - val_acc: 0.7045\n",
            "Epoch 137/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3112 - acc: 0.8842 - val_loss: 0.9762 - val_acc: 0.6832\n",
            "Epoch 138/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.3185 - acc: 0.8806 - val_loss: 1.0069 - val_acc: 0.6643\n",
            "Epoch 139/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3079 - acc: 0.8861 - val_loss: 0.9313 - val_acc: 0.6856\n",
            "Epoch 140/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3153 - acc: 0.8762 - val_loss: 0.9885 - val_acc: 0.6809\n",
            "Epoch 141/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3271 - acc: 0.8774 - val_loss: 1.0096 - val_acc: 0.6714\n",
            "Epoch 142/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3043 - acc: 0.8864 - val_loss: 0.9770 - val_acc: 0.6832\n",
            "Epoch 143/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.3025 - acc: 0.8848 - val_loss: 1.0429 - val_acc: 0.6738\n",
            "Epoch 144/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3159 - acc: 0.8853 - val_loss: 1.0115 - val_acc: 0.6761\n",
            "Epoch 145/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3079 - acc: 0.8824 - val_loss: 0.9650 - val_acc: 0.6927\n",
            "Epoch 146/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2919 - acc: 0.8898 - val_loss: 0.9790 - val_acc: 0.6903\n",
            "Epoch 147/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2934 - acc: 0.8887 - val_loss: 1.0434 - val_acc: 0.6927\n",
            "Epoch 148/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2992 - acc: 0.8910 - val_loss: 0.9855 - val_acc: 0.6856\n",
            "Epoch 149/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.3047 - acc: 0.8874 - val_loss: 1.0197 - val_acc: 0.6974\n",
            "Epoch 150/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2966 - acc: 0.8874 - val_loss: 0.9958 - val_acc: 0.7069\n",
            "Epoch 151/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3033 - acc: 0.8887 - val_loss: 1.0944 - val_acc: 0.6809\n",
            "Epoch 152/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3074 - acc: 0.8831 - val_loss: 0.9287 - val_acc: 0.6974\n",
            "Epoch 153/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3059 - acc: 0.8867 - val_loss: 1.0891 - val_acc: 0.6619\n",
            "Epoch 154/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2965 - acc: 0.8935 - val_loss: 0.9492 - val_acc: 0.6903\n",
            "Epoch 155/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2969 - acc: 0.8895 - val_loss: 1.0419 - val_acc: 0.6879\n",
            "Epoch 156/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2933 - acc: 0.8896 - val_loss: 0.9768 - val_acc: 0.7021\n",
            "Epoch 157/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.3019 - acc: 0.8882 - val_loss: 1.0277 - val_acc: 0.6785\n",
            "Epoch 158/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2928 - acc: 0.8865 - val_loss: 0.9995 - val_acc: 0.6832\n",
            "Epoch 159/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2961 - acc: 0.8879 - val_loss: 1.0329 - val_acc: 0.6761\n",
            "Epoch 160/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2861 - acc: 0.8947 - val_loss: 1.0504 - val_acc: 0.6809\n",
            "Epoch 161/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2826 - acc: 0.8942 - val_loss: 1.0354 - val_acc: 0.6974\n",
            "Epoch 162/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2901 - acc: 0.8917 - val_loss: 1.0123 - val_acc: 0.6832\n",
            "Epoch 163/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2843 - acc: 0.8952 - val_loss: 1.0805 - val_acc: 0.6738\n",
            "Epoch 164/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2951 - acc: 0.8859 - val_loss: 0.9685 - val_acc: 0.6879\n",
            "Epoch 165/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2809 - acc: 0.8964 - val_loss: 0.9778 - val_acc: 0.6856\n",
            "Epoch 166/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2840 - acc: 0.8933 - val_loss: 1.0123 - val_acc: 0.6832\n",
            "Epoch 167/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2831 - acc: 0.8924 - val_loss: 1.0129 - val_acc: 0.6998\n",
            "Epoch 168/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2840 - acc: 0.8930 - val_loss: 1.0091 - val_acc: 0.6927\n",
            "Epoch 169/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2953 - acc: 0.8911 - val_loss: 0.9829 - val_acc: 0.7163\n",
            "Epoch 170/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2967 - acc: 0.8911 - val_loss: 0.9596 - val_acc: 0.7116\n",
            "Epoch 171/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2909 - acc: 0.8967 - val_loss: 1.0154 - val_acc: 0.6714\n",
            "Epoch 172/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2823 - acc: 0.8961 - val_loss: 0.9593 - val_acc: 0.6879\n",
            "Epoch 173/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2977 - acc: 0.8868 - val_loss: 1.0032 - val_acc: 0.6927\n",
            "Epoch 174/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2931 - acc: 0.8911 - val_loss: 1.0151 - val_acc: 0.6998\n",
            "Epoch 175/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2961 - acc: 0.8889 - val_loss: 0.9591 - val_acc: 0.6903\n",
            "Epoch 176/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2650 - acc: 0.9026 - val_loss: 0.9930 - val_acc: 0.6974\n",
            "Epoch 177/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2790 - acc: 0.8942 - val_loss: 1.0119 - val_acc: 0.6879\n",
            "Epoch 178/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2815 - acc: 0.8972 - val_loss: 1.0167 - val_acc: 0.6927\n",
            "Epoch 179/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2655 - acc: 0.9003 - val_loss: 1.0486 - val_acc: 0.6832\n",
            "Epoch 180/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2784 - acc: 0.8936 - val_loss: 1.0687 - val_acc: 0.6879\n",
            "Epoch 181/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2809 - acc: 0.8976 - val_loss: 0.9945 - val_acc: 0.6974\n",
            "Epoch 182/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2821 - acc: 0.8947 - val_loss: 1.0088 - val_acc: 0.6690\n",
            "Epoch 183/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2647 - acc: 0.9016 - val_loss: 1.0744 - val_acc: 0.6690\n",
            "Epoch 184/200\n",
            "106/106 [==============================] - 2s 15ms/step - loss: 0.2737 - acc: 0.8972 - val_loss: 1.0361 - val_acc: 0.6761\n",
            "Epoch 185/200\n",
            "106/106 [==============================] - 2s 15ms/step - loss: 0.2750 - acc: 0.8963 - val_loss: 0.9907 - val_acc: 0.6809\n",
            "Epoch 186/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2848 - acc: 0.8933 - val_loss: 0.9760 - val_acc: 0.6879\n",
            "Epoch 187/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2712 - acc: 0.9037 - val_loss: 0.9988 - val_acc: 0.6950\n",
            "Epoch 188/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2701 - acc: 0.9028 - val_loss: 1.1248 - val_acc: 0.6785\n",
            "Epoch 189/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2695 - acc: 0.8982 - val_loss: 1.0484 - val_acc: 0.6785\n",
            "Epoch 190/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2658 - acc: 0.9010 - val_loss: 1.0168 - val_acc: 0.7021\n",
            "Epoch 191/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2759 - acc: 0.9006 - val_loss: 0.9990 - val_acc: 0.6832\n",
            "Epoch 192/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2589 - acc: 0.9016 - val_loss: 1.0322 - val_acc: 0.6548\n",
            "Epoch 193/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2669 - acc: 0.9038 - val_loss: 1.0083 - val_acc: 0.6927\n",
            "Epoch 194/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2676 - acc: 0.8982 - val_loss: 1.0276 - val_acc: 0.6950\n",
            "Epoch 195/200\n",
            "106/106 [==============================] - 1s 14ms/step - loss: 0.2635 - acc: 0.9016 - val_loss: 1.0629 - val_acc: 0.6714\n",
            "Epoch 196/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2786 - acc: 0.8963 - val_loss: 1.0276 - val_acc: 0.6785\n",
            "Epoch 197/200\n",
            "106/106 [==============================] - 2s 14ms/step - loss: 0.2684 - acc: 0.9012 - val_loss: 1.1022 - val_acc: 0.6572\n",
            "Epoch 198/200\n",
            "106/106 [==============================] - 2s 15ms/step - loss: 0.2709 - acc: 0.8967 - val_loss: 1.0374 - val_acc: 0.6832\n",
            "Epoch 199/200\n",
            "106/106 [==============================] - 2s 15ms/step - loss: 0.2722 - acc: 0.8942 - val_loss: 1.0280 - val_acc: 0.6903\n",
            "Epoch 200/200\n",
            "106/106 [==============================] - 2s 15ms/step - loss: 0.2736 - acc: 0.9004 - val_loss: 1.0064 - val_acc: 0.6950\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "test_loss, test_acc = model.evaluate(X_spatial_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "E5W7-Qi6vG_9",
        "outputId": "6feafd67-fbb2-46a8-a0c8-0b4d097db9c0"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 6ms/step - loss: 0.9697 - acc: 0.7201\n",
            "Test accuracy 0.7200902700424194\n",
            "Test loss 0.969683825969696\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['acc'], label='train')\n",
        "plt.plot(history.history['val_acc'], label='val')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "qS3hSLpYWYAa",
        "outputId": "b7b64567-ed75-4902-8d09-872a7c9b28b4"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the model on entire data\n",
        "We now train the optimised model from the previous section on the data on the entire data, i.e both train and val sets. Note: we have also saved the architecture of the model as a SavedModel file from Colab, and are including it along with the submission"
      ],
      "metadata": {
        "id": "quT1aOWLWaS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all,y_train_all= data_prep(X_train_valid,y_train_valid,2,2,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CymOAvmyeko3",
        "outputId": "521bd548-4c1d-4290-ddc7-cfccb9c205f6"
      },
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (2115, 22, 500)\n",
            "Shape of X after maxpooling: (2115, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (4230, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (8460, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_all = spatial_prep(X_train_all)"
      ],
      "metadata": {
        "id": "lkyCscsvDerp"
      },
      "execution_count": 216,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_all = keras.utils.to_categorical(y_train_all, num_classes)"
      ],
      "metadata": {
        "id": "d-xe5hCKexlM"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_all = np.swapaxes(X_spatial_all,1,3)\n",
        "X_spatial_all = np.swapaxes(X_spatial_all,2,3)"
      ],
      "metadata": {
        "id": "MPeZG57UDhPI"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_spatial_all.shape, y_train_all.shape, y_test.shape, X_spatial_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eKg-wb2aDjoA",
        "outputId": "4f493a49-5e2e-42ad-96db-40e8a58ae27c"
      },
      "execution_count": 219,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(8460, 250, 6, 7) (8460, 4) (443, 4) (443, 250, 6, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "opt = keras.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "fbNufw_96wwT"
      },
      "execution_count": 265,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "callbacks = keras.callbacks.ReduceLROnPlateau(\n",
        "  monitor='loss', factor=0.1, patience=10,\n",
        "  mode='auto', min_delta=0.0001, cooldown=0, min_lr=0,\n",
        ")"
      ],
      "metadata": {
        "id": "nxnNHIUkFJHY"
      },
      "execution_count": 266,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "history = model.fit(X_spatial_all, y_train_all, batch_size=64, epochs=200, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "B6E65haLe7VH",
        "outputId": "d217ba47-8c56-4300-8288-f86316f4727f"
      },
      "execution_count": 230,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "133/133 [==============================] - 5s 14ms/step - loss: 1.3841 - acc: 0.3191 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.2639 - acc: 0.4071 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.1880 - acc: 0.4730 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.1351 - acc: 0.5129 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.0891 - acc: 0.5320 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.0515 - acc: 0.5557 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 1.0274 - acc: 0.5735 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.9866 - acc: 0.5907 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.9444 - acc: 0.6147 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.9213 - acc: 0.6271 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.9033 - acc: 0.6292 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.8766 - acc: 0.6447 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.8454 - acc: 0.6593 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.8133 - acc: 0.6738 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7949 - acc: 0.6844 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7840 - acc: 0.6780 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7770 - acc: 0.6905 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7550 - acc: 0.7017 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7310 - acc: 0.7115 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7329 - acc: 0.7086 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.7094 - acc: 0.7174 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6980 - acc: 0.7221 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6812 - acc: 0.7342 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6756 - acc: 0.7400 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6733 - acc: 0.7351 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6575 - acc: 0.7384 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6479 - acc: 0.7468 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6357 - acc: 0.7570 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6246 - acc: 0.7589 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6281 - acc: 0.7518 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.6119 - acc: 0.7677 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5927 - acc: 0.7742 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5865 - acc: 0.7728 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5884 - acc: 0.7663 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5877 - acc: 0.7687 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5743 - acc: 0.7738 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5677 - acc: 0.7830 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5649 - acc: 0.7842 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5518 - acc: 0.7909 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5526 - acc: 0.7889 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5498 - acc: 0.7888 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5392 - acc: 0.7877 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5309 - acc: 0.7940 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5321 - acc: 0.7953 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5314 - acc: 0.7930 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4983 - acc: 0.8100 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5149 - acc: 0.7982 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4981 - acc: 0.8078 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5032 - acc: 0.8071 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.5038 - acc: 0.8087 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.5016 - acc: 0.8053 - lr: 0.0010\n",
            "Epoch 52/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4904 - acc: 0.8097 - lr: 0.0010\n",
            "Epoch 53/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4976 - acc: 0.8079 - lr: 0.0010\n",
            "Epoch 54/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4836 - acc: 0.8125 - lr: 0.0010\n",
            "Epoch 55/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4721 - acc: 0.8173 - lr: 0.0010\n",
            "Epoch 56/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4793 - acc: 0.8139 - lr: 0.0010\n",
            "Epoch 57/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4801 - acc: 0.8169 - lr: 0.0010\n",
            "Epoch 58/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4661 - acc: 0.8199 - lr: 0.0010\n",
            "Epoch 59/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4697 - acc: 0.8207 - lr: 0.0010\n",
            "Epoch 60/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4768 - acc: 0.8201 - lr: 0.0010\n",
            "Epoch 61/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4626 - acc: 0.8257 - lr: 0.0010\n",
            "Epoch 62/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4467 - acc: 0.8285 - lr: 0.0010\n",
            "Epoch 63/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4537 - acc: 0.8248 - lr: 0.0010\n",
            "Epoch 64/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4583 - acc: 0.8254 - lr: 0.0010\n",
            "Epoch 65/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4627 - acc: 0.8243 - lr: 0.0010\n",
            "Epoch 66/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4508 - acc: 0.8267 - lr: 0.0010\n",
            "Epoch 67/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4317 - acc: 0.8349 - lr: 0.0010\n",
            "Epoch 68/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4392 - acc: 0.8312 - lr: 0.0010\n",
            "Epoch 69/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4475 - acc: 0.8270 - lr: 0.0010\n",
            "Epoch 70/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4495 - acc: 0.8288 - lr: 0.0010\n",
            "Epoch 71/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4246 - acc: 0.8353 - lr: 0.0010\n",
            "Epoch 72/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4278 - acc: 0.8355 - lr: 0.0010\n",
            "Epoch 73/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4273 - acc: 0.8356 - lr: 0.0010\n",
            "Epoch 74/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4276 - acc: 0.8343 - lr: 0.0010\n",
            "Epoch 75/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4180 - acc: 0.8428 - lr: 0.0010\n",
            "Epoch 76/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4238 - acc: 0.8394 - lr: 0.0010\n",
            "Epoch 77/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4203 - acc: 0.8410 - lr: 0.0010\n",
            "Epoch 78/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4186 - acc: 0.8368 - lr: 0.0010\n",
            "Epoch 79/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4221 - acc: 0.8410 - lr: 0.0010\n",
            "Epoch 80/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4136 - acc: 0.8424 - lr: 0.0010\n",
            "Epoch 81/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4189 - acc: 0.8390 - lr: 0.0010\n",
            "Epoch 82/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.4265 - acc: 0.8370 - lr: 0.0010\n",
            "Epoch 83/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3936 - acc: 0.8494 - lr: 0.0010\n",
            "Epoch 84/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.4089 - acc: 0.8442 - lr: 0.0010\n",
            "Epoch 85/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3926 - acc: 0.8544 - lr: 0.0010\n",
            "Epoch 86/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3994 - acc: 0.8513 - lr: 0.0010\n",
            "Epoch 87/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3923 - acc: 0.8502 - lr: 0.0010\n",
            "Epoch 88/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3857 - acc: 0.8532 - lr: 0.0010\n",
            "Epoch 89/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3871 - acc: 0.8500 - lr: 0.0010\n",
            "Epoch 90/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3950 - acc: 0.8501 - lr: 0.0010\n",
            "Epoch 91/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3887 - acc: 0.8500 - lr: 0.0010\n",
            "Epoch 92/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3750 - acc: 0.8552 - lr: 0.0010\n",
            "Epoch 93/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3988 - acc: 0.8489 - lr: 0.0010\n",
            "Epoch 94/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3815 - acc: 0.8526 - lr: 0.0010\n",
            "Epoch 95/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3787 - acc: 0.8517 - lr: 0.0010\n",
            "Epoch 96/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3792 - acc: 0.8543 - lr: 0.0010\n",
            "Epoch 97/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3835 - acc: 0.8526 - lr: 0.0010\n",
            "Epoch 98/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3908 - acc: 0.8548 - lr: 0.0010\n",
            "Epoch 99/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3821 - acc: 0.8540 - lr: 0.0010\n",
            "Epoch 100/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3585 - acc: 0.8656 - lr: 0.0010\n",
            "Epoch 101/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3679 - acc: 0.8600 - lr: 0.0010\n",
            "Epoch 102/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3635 - acc: 0.8642 - lr: 0.0010\n",
            "Epoch 103/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3585 - acc: 0.8631 - lr: 0.0010\n",
            "Epoch 104/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3564 - acc: 0.8635 - lr: 0.0010\n",
            "Epoch 105/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3530 - acc: 0.8691 - lr: 0.0010\n",
            "Epoch 106/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3649 - acc: 0.8608 - lr: 0.0010\n",
            "Epoch 107/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3674 - acc: 0.8604 - lr: 0.0010\n",
            "Epoch 108/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3507 - acc: 0.8650 - lr: 0.0010\n",
            "Epoch 109/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3600 - acc: 0.8612 - lr: 0.0010\n",
            "Epoch 110/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3654 - acc: 0.8613 - lr: 0.0010\n",
            "Epoch 111/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3711 - acc: 0.8567 - lr: 0.0010\n",
            "Epoch 112/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3504 - acc: 0.8665 - lr: 0.0010\n",
            "Epoch 113/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3503 - acc: 0.8673 - lr: 0.0010\n",
            "Epoch 114/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3473 - acc: 0.8689 - lr: 0.0010\n",
            "Epoch 115/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3509 - acc: 0.8674 - lr: 0.0010\n",
            "Epoch 116/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3644 - acc: 0.8606 - lr: 0.0010\n",
            "Epoch 117/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3738 - acc: 0.8592 - lr: 0.0010\n",
            "Epoch 118/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3429 - acc: 0.8675 - lr: 0.0010\n",
            "Epoch 119/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3478 - acc: 0.8667 - lr: 0.0010\n",
            "Epoch 120/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3441 - acc: 0.8691 - lr: 0.0010\n",
            "Epoch 121/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3394 - acc: 0.8715 - lr: 0.0010\n",
            "Epoch 122/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3477 - acc: 0.8682 - lr: 0.0010\n",
            "Epoch 123/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3407 - acc: 0.8708 - lr: 0.0010\n",
            "Epoch 124/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3387 - acc: 0.8690 - lr: 0.0010\n",
            "Epoch 125/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3465 - acc: 0.8674 - lr: 0.0010\n",
            "Epoch 126/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3331 - acc: 0.8721 - lr: 0.0010\n",
            "Epoch 127/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3341 - acc: 0.8726 - lr: 0.0010\n",
            "Epoch 128/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3204 - acc: 0.8837 - lr: 0.0010\n",
            "Epoch 129/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3317 - acc: 0.8743 - lr: 0.0010\n",
            "Epoch 130/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3326 - acc: 0.8757 - lr: 0.0010\n",
            "Epoch 131/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3341 - acc: 0.8741 - lr: 0.0010\n",
            "Epoch 132/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3287 - acc: 0.8772 - lr: 0.0010\n",
            "Epoch 133/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3372 - acc: 0.8706 - lr: 0.0010\n",
            "Epoch 134/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3386 - acc: 0.8687 - lr: 0.0010\n",
            "Epoch 135/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3381 - acc: 0.8726 - lr: 0.0010\n",
            "Epoch 136/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3193 - acc: 0.8823 - lr: 0.0010\n",
            "Epoch 137/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3242 - acc: 0.8773 - lr: 0.0010\n",
            "Epoch 138/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3394 - acc: 0.8687 - lr: 0.0010\n",
            "Epoch 139/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3186 - acc: 0.8807 - lr: 0.0010\n",
            "Epoch 140/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3134 - acc: 0.8823 - lr: 0.0010\n",
            "Epoch 141/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3403 - acc: 0.8695 - lr: 0.0010\n",
            "Epoch 142/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3313 - acc: 0.8755 - lr: 0.0010\n",
            "Epoch 143/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3127 - acc: 0.8852 - lr: 0.0010\n",
            "Epoch 144/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3160 - acc: 0.8817 - lr: 0.0010\n",
            "Epoch 145/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3210 - acc: 0.8785 - lr: 0.0010\n",
            "Epoch 146/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3163 - acc: 0.8835 - lr: 0.0010\n",
            "Epoch 147/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3195 - acc: 0.8800 - lr: 0.0010\n",
            "Epoch 148/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3191 - acc: 0.8772 - lr: 0.0010\n",
            "Epoch 149/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3067 - acc: 0.8835 - lr: 0.0010\n",
            "Epoch 150/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3133 - acc: 0.8819 - lr: 0.0010\n",
            "Epoch 151/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3123 - acc: 0.8855 - lr: 0.0010\n",
            "Epoch 152/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3194 - acc: 0.8803 - lr: 0.0010\n",
            "Epoch 153/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3113 - acc: 0.8812 - lr: 0.0010\n",
            "Epoch 154/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3228 - acc: 0.8794 - lr: 0.0010\n",
            "Epoch 155/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3030 - acc: 0.8863 - lr: 0.0010\n",
            "Epoch 156/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3077 - acc: 0.8859 - lr: 0.0010\n",
            "Epoch 157/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3092 - acc: 0.8848 - lr: 0.0010\n",
            "Epoch 158/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3055 - acc: 0.8849 - lr: 0.0010\n",
            "Epoch 159/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3100 - acc: 0.8832 - lr: 0.0010\n",
            "Epoch 160/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3053 - acc: 0.8869 - lr: 0.0010\n",
            "Epoch 161/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3050 - acc: 0.8839 - lr: 0.0010\n",
            "Epoch 162/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3026 - acc: 0.8838 - lr: 0.0010\n",
            "Epoch 163/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3159 - acc: 0.8838 - lr: 0.0010\n",
            "Epoch 164/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3122 - acc: 0.8872 - lr: 0.0010\n",
            "Epoch 165/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3079 - acc: 0.8839 - lr: 0.0010\n",
            "Epoch 166/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3186 - acc: 0.8778 - lr: 0.0010\n",
            "Epoch 167/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3175 - acc: 0.8803 - lr: 0.0010\n",
            "Epoch 168/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3039 - acc: 0.8836 - lr: 0.0010\n",
            "Epoch 169/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2951 - acc: 0.8902 - lr: 0.0010\n",
            "Epoch 170/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2946 - acc: 0.8911 - lr: 0.0010\n",
            "Epoch 171/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3039 - acc: 0.8861 - lr: 0.0010\n",
            "Epoch 172/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2881 - acc: 0.8909 - lr: 0.0010\n",
            "Epoch 173/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3002 - acc: 0.8879 - lr: 0.0010\n",
            "Epoch 174/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2896 - acc: 0.8931 - lr: 0.0010\n",
            "Epoch 175/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2948 - acc: 0.8870 - lr: 0.0010\n",
            "Epoch 176/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2885 - acc: 0.8927 - lr: 0.0010\n",
            "Epoch 177/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3081 - acc: 0.8813 - lr: 0.0010\n",
            "Epoch 178/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3062 - acc: 0.8871 - lr: 0.0010\n",
            "Epoch 179/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.3057 - acc: 0.8837 - lr: 0.0010\n",
            "Epoch 180/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2872 - acc: 0.8895 - lr: 0.0010\n",
            "Epoch 181/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2981 - acc: 0.8885 - lr: 0.0010\n",
            "Epoch 182/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2982 - acc: 0.8897 - lr: 0.0010\n",
            "Epoch 183/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.3076 - acc: 0.8829 - lr: 0.0010\n",
            "Epoch 184/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2992 - acc: 0.8863 - lr: 0.0010\n",
            "Epoch 185/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2801 - acc: 0.8924 - lr: 0.0010\n",
            "Epoch 186/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2884 - acc: 0.8913 - lr: 0.0010\n",
            "Epoch 187/200\n",
            "133/133 [==============================] - 2s 16ms/step - loss: 0.2991 - acc: 0.8852 - lr: 0.0010\n",
            "Epoch 188/200\n",
            "133/133 [==============================] - 3s 20ms/step - loss: 0.2741 - acc: 0.8972 - lr: 0.0010\n",
            "Epoch 189/200\n",
            "133/133 [==============================] - 2s 15ms/step - loss: 0.2930 - acc: 0.8947 - lr: 0.0010\n",
            "Epoch 190/200\n",
            "133/133 [==============================] - 2s 17ms/step - loss: 0.2864 - acc: 0.8941 - lr: 0.0010\n",
            "Epoch 191/200\n",
            "133/133 [==============================] - 3s 21ms/step - loss: 0.2804 - acc: 0.8967 - lr: 0.0010\n",
            "Epoch 192/200\n",
            "133/133 [==============================] - 2s 18ms/step - loss: 0.2770 - acc: 0.8959 - lr: 0.0010\n",
            "Epoch 193/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2849 - acc: 0.8952 - lr: 0.0010\n",
            "Epoch 194/200\n",
            "133/133 [==============================] - 2s 13ms/step - loss: 0.2733 - acc: 0.8986 - lr: 0.0010\n",
            "Epoch 195/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2887 - acc: 0.8924 - lr: 0.0010\n",
            "Epoch 196/200\n",
            "133/133 [==============================] - 2s 17ms/step - loss: 0.2802 - acc: 0.8949 - lr: 0.0010\n",
            "Epoch 197/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2723 - acc: 0.8955 - lr: 0.0010\n",
            "Epoch 198/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2816 - acc: 0.8940 - lr: 0.0010\n",
            "Epoch 199/200\n",
            "133/133 [==============================] - 2s 14ms/step - loss: 0.2844 - acc: 0.8947 - lr: 0.0010\n",
            "Epoch 200/200\n",
            "133/133 [==============================] - 2s 16ms/step - loss: 0.2823 - acc: 0.8962 - lr: 0.0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# giga = keras.models.load_model('final-model')\n",
        "# model = giga\n",
        "test_loss, test_acc = model.evaluate(X_spatial_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc)\n",
        "print(\"Test loss\", test_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M5GUKfV6TyHI",
        "outputId": "68455221-4600-435d-e31d-4de92623ede9"
      },
      "execution_count": 290,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 7ms/step - loss: 0.8571 - acc: 0.7698\n",
            "Test accuracy 0.7697516679763794\n",
            "Test loss 0.8571227788925171\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To assess how the model performed on different classes, we plot a confusion matrix to compute class performance"
      ],
      "metadata": {
        "id": "vnvYiWMXCja-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_spatial_test)"
      ],
      "metadata": {
        "id": "xw7nT5hDKz2c"
      },
      "execution_count": 291,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_pred.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3i2Ru_iMK5lm",
        "outputId": "891aeab9-a30f-4b6e-c744-c56f15ddff86"
      },
      "execution_count": 292,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(443, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay "
      ],
      "metadata": {
        "id": "-V4mKBWXLGwS"
      },
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix = confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))"
      ],
      "metadata": {
        "id": "kSTTq98KLIfP"
      },
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_disp = ConfusionMatrixDisplay(matrix, display_labels=[769, 770, 771, 772]) \n",
        "log_disp.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "5Oxo0kMmKtN9",
        "outputId": "9abb31ed-f5e9-4b33-beef-da08c8fed9f6"
      },
      "execution_count": 295,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f1afada27d0>"
            ]
          },
          "metadata": {},
          "execution_count": 295
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('final-model')"
      ],
      "metadata": {
        "id": "APhnh7owuZGx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2e856e8d-8c6a-4eb6-fdf6-079fa5f61c05"
      },
      "execution_count": 296,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as gru_cell_32_layer_call_fn, gru_cell_32_layer_call_and_return_conditional_losses, gru_cell_33_layer_call_fn, gru_cell_33_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: final-model/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: final-model/assets\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1c4a252050> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.GRUCell object at 0x7f1c34265250> has the same name 'GRUCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.GRUCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(9):\n",
        "  indeces = [ i for i in range(len(person_test)) if person_test[i]==j]\n",
        "  X_subject= X_test[indeces]\n",
        "  y_subject = y_test[indeces]\n",
        "  X_spatial_subject = spatial_prep(X_subject)\n",
        "  X_spatial_subject = np.swapaxes(X_spatial_subject, 1,3)\n",
        "  X_spatial_subject = np.swapaxes(X_spatial_subject, 2,3)\n",
        "\n",
        "  test_loss_subject, test_acc_subject = model.evaluate(X_spatial_subject, y_subject)\n",
        "  print(\"Statistics for subject \", j)\n",
        "  print(\"Test accuracy\", test_acc_subject)\n",
        "  print(\"Test loss\", test_loss_subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z26CllGtPMCZ",
        "outputId": "7572c91d-a7fe-4fb0-9873-81378b698226"
      },
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1200 - acc: 0.6800\n",
            "Statistics for subject  0\n",
            "Test accuracy 0.6800000071525574\n",
            "Test loss 1.1200186014175415\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7176 - acc: 0.7800\n",
            "Statistics for subject  1\n",
            "Test accuracy 0.7799999713897705\n",
            "Test loss 0.7175740599632263\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7058 - acc: 0.8000\n",
            "Statistics for subject  2\n",
            "Test accuracy 0.800000011920929\n",
            "Test loss 0.7058024406433105\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8001 - acc: 0.7600\n",
            "Statistics for subject  3\n",
            "Test accuracy 0.7599999904632568\n",
            "Test loss 0.800101637840271\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1086 - acc: 0.7021\n",
            "Statistics for subject  4\n",
            "Test accuracy 0.7021276354789734\n",
            "Test loss 1.1085593700408936\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8561 - acc: 0.7551\n",
            "Statistics for subject  5\n",
            "Test accuracy 0.7551020383834839\n",
            "Test loss 0.8561388850212097\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8277 - acc: 0.8200\n",
            "Statistics for subject  6\n",
            "Test accuracy 0.8199999928474426\n",
            "Test loss 0.8277008533477783\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.0984 - acc: 0.7600\n",
            "Statistics for subject  7\n",
            "Test accuracy 0.7599999904632568\n",
            "Test loss 1.0984420776367188\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4717 - acc: 0.8723\n",
            "Statistics for subject  8\n",
            "Test accuracy 0.8723404407501221\n",
            "Test loss 0.47170859575271606\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training the model for one subject\n",
        "We now train the model architecture on just one subject, and compute the test accuracies for both the subject in question, and for all subjects"
      ],
      "metadata": {
        "id": "l-4_tqTVC3n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_indeces = [ i for i in range(len(person_train_valid)) if person_train_valid[i]==0]\n",
        "X_train_subject_1 = X_train_valid[train_indeces]\n",
        "y_train_subject_1 = y_train_valid[train_indeces]"
      ],
      "metadata": {
        "id": "AwN_EjMMti8n"
      },
      "execution_count": 244,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_indeces = [ i for i in range(len(person_test)) if person_test[i]==0]\n",
        "X_test_subject_1 = X_test[test_indeces]\n",
        "y_test_subject_1 = y_test[test_indeces]\n"
      ],
      "metadata": {
        "id": "mSKHoRgJtjqr"
      },
      "execution_count": 245,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_subject_1.shape, y_train_subject_1.shape, X_test_subject_1.shape, y_test_subject_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HihoFsvPts1H",
        "outputId": "103055ac-1a1f-45ce-bc81-4a35f6db23d2"
      },
      "execution_count": 246,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(237, 22, 1000) (237,) (50, 22, 250) (50, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_1,y_train_1= data_prep(X_train_subject_1,y_train_subject_1,2,2,True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "tgGSl_7CuQPO",
        "outputId": "c00a7878-4090-48b8-e5ac-6cf17f3234a0"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X after trimming: (237, 22, 500)\n",
            "Shape of X after maxpooling: (237, 22, 250)\n",
            "Shape of X after averaging+noise and concatenating: (474, 22, 250)\n",
            "Shape of X after subsampling and concatenating: (948, 22, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_train_1 = spatial_prep(X_train_1)"
      ],
      "metadata": {
        "id": "EO8alELqugZ5"
      },
      "execution_count": 248,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_test_1 = spatial_prep(X_test_subject_1)"
      ],
      "metadata": {
        "id": "jXNGs6ndukEN"
      },
      "execution_count": 249,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_spatial_train_1.shape,X_spatial_test_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "80mA-w3Vuth8",
        "outputId": "fa295cee-03ef-43f2-c314-7a3c1b2eab0a"
      },
      "execution_count": 250,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(948, 6, 7, 250) (50, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_1.shape,y_train_1.shape, X_spatial_train_1.shape)\n",
        "print(X_test_subject_1.shape, y_test_subject_1.shape, X_spatial_test_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3tIvejqEvQJi",
        "outputId": "b4fc76f4-3e80-4222-d450-58e02b58af6c"
      },
      "execution_count": 251,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(948, 22, 250) (948,) (948, 6, 7, 250)\n",
            "(50, 22, 250) (50, 4) (50, 6, 7, 250)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_spatial_train_1 = np.swapaxes(X_spatial_train_1 , 1,3)\n",
        "X_spatial_train_1 = np.swapaxes(X_spatial_train_1 , 2,3)\n",
        "X_spatial_test_1  = np.swapaxes(X_spatial_test_1, 1,3)\n",
        "X_spatial_test_1 = np.swapaxes(X_spatial_test_1 , 2,3)"
      ],
      "metadata": {
        "id": "hFjPC8XNvYW3"
      },
      "execution_count": 252,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train_1 = keras.utils.to_categorical(y_train_1, num_classes)"
      ],
      "metadata": {
        "id": "XwHCw7DzxAHo"
      },
      "execution_count": 256,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(X_train_1.shape,y_train_1.shape, X_spatial_train_1.shape)\n",
        "print(X_test_subject_1.shape, y_test_subject_1.shape, X_spatial_test_1.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3UITVGDtvaYO",
        "outputId": "0def01f4-2407-4132-bf4c-0cc74951abd4"
      },
      "execution_count": 257,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(948, 22, 250) (948, 4) (948, 250, 6, 7)\n",
            "(50, 22, 250) (50, 4) (50, 250, 6, 7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])\n",
        "history = model.fit(X_spatial_train_1, y_train_1, batch_size=64, epochs=100, callbacks = callbacks)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "_puS7U-UwTeG",
        "outputId": "57ea3c8f-3716-405d-db60-15906082075e"
      },
      "execution_count": 267,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "15/15 [==============================] - 3s 20ms/step - loss: 1.4719 - acc: 0.2416 - lr: 0.0010\n",
            "Epoch 2/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.3926 - acc: 0.3312 - lr: 0.0010\n",
            "Epoch 3/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.3768 - acc: 0.3186 - lr: 0.0010\n",
            "Epoch 4/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.3434 - acc: 0.3576 - lr: 0.0010\n",
            "Epoch 5/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.2801 - acc: 0.4177 - lr: 0.0010\n",
            "Epoch 6/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.2427 - acc: 0.4325 - lr: 0.0010\n",
            "Epoch 7/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.1909 - acc: 0.4747 - lr: 0.0010\n",
            "Epoch 8/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 1.1651 - acc: 0.4916 - lr: 0.0010\n",
            "Epoch 9/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.0964 - acc: 0.5158 - lr: 0.0010\n",
            "Epoch 10/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 1.0203 - acc: 0.5612 - lr: 0.0010\n",
            "Epoch 11/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.9577 - acc: 0.6181 - lr: 0.0010\n",
            "Epoch 12/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.8847 - acc: 0.6319 - lr: 0.0010\n",
            "Epoch 13/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.8502 - acc: 0.6656 - lr: 0.0010\n",
            "Epoch 14/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.7632 - acc: 0.6920 - lr: 0.0010\n",
            "Epoch 15/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.7563 - acc: 0.6909 - lr: 0.0010\n",
            "Epoch 16/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6936 - acc: 0.7226 - lr: 0.0010\n",
            "Epoch 17/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6401 - acc: 0.7479 - lr: 0.0010\n",
            "Epoch 18/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.6096 - acc: 0.7574 - lr: 0.0010\n",
            "Epoch 19/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.5215 - acc: 0.8059 - lr: 0.0010\n",
            "Epoch 20/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4854 - acc: 0.8196 - lr: 0.0010\n",
            "Epoch 21/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4837 - acc: 0.7975 - lr: 0.0010\n",
            "Epoch 22/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.4694 - acc: 0.8186 - lr: 0.0010\n",
            "Epoch 23/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.4466 - acc: 0.8449 - lr: 0.0010\n",
            "Epoch 24/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3733 - acc: 0.8618 - lr: 0.0010\n",
            "Epoch 25/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3716 - acc: 0.8745 - lr: 0.0010\n",
            "Epoch 26/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3245 - acc: 0.8797 - lr: 0.0010\n",
            "Epoch 27/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.3944 - acc: 0.8365 - lr: 0.0010\n",
            "Epoch 28/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3634 - acc: 0.8565 - lr: 0.0010\n",
            "Epoch 29/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3294 - acc: 0.8755 - lr: 0.0010\n",
            "Epoch 30/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3169 - acc: 0.8892 - lr: 0.0010\n",
            "Epoch 31/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3241 - acc: 0.8724 - lr: 0.0010\n",
            "Epoch 32/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.3054 - acc: 0.8755 - lr: 0.0010\n",
            "Epoch 33/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2916 - acc: 0.8861 - lr: 0.0010\n",
            "Epoch 34/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2724 - acc: 0.8977 - lr: 0.0010\n",
            "Epoch 35/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2616 - acc: 0.8924 - lr: 0.0010\n",
            "Epoch 36/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2471 - acc: 0.9093 - lr: 0.0010\n",
            "Epoch 37/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2142 - acc: 0.9230 - lr: 0.0010\n",
            "Epoch 38/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2274 - acc: 0.9167 - lr: 0.0010\n",
            "Epoch 39/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2167 - acc: 0.9209 - lr: 0.0010\n",
            "Epoch 40/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2061 - acc: 0.9283 - lr: 0.0010\n",
            "Epoch 41/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.2106 - acc: 0.9272 - lr: 0.0010\n",
            "Epoch 42/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2237 - acc: 0.9177 - lr: 0.0010\n",
            "Epoch 43/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2244 - acc: 0.9093 - lr: 0.0010\n",
            "Epoch 44/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1909 - acc: 0.9262 - lr: 0.0010\n",
            "Epoch 45/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.2166 - acc: 0.9135 - lr: 0.0010\n",
            "Epoch 46/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1622 - acc: 0.9430 - lr: 0.0010\n",
            "Epoch 47/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1773 - acc: 0.9283 - lr: 0.0010\n",
            "Epoch 48/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1743 - acc: 0.9314 - lr: 0.0010\n",
            "Epoch 49/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1604 - acc: 0.9420 - lr: 0.0010\n",
            "Epoch 50/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1472 - acc: 0.9462 - lr: 0.0010\n",
            "Epoch 51/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1416 - acc: 0.9483 - lr: 0.0010\n",
            "Epoch 52/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1533 - acc: 0.9494 - lr: 0.0010\n",
            "Epoch 53/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1540 - acc: 0.9430 - lr: 0.0010\n",
            "Epoch 54/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1377 - acc: 0.9515 - lr: 0.0010\n",
            "Epoch 55/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1397 - acc: 0.9504 - lr: 0.0010\n",
            "Epoch 56/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1195 - acc: 0.9589 - lr: 0.0010\n",
            "Epoch 57/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1577 - acc: 0.9399 - lr: 0.0010\n",
            "Epoch 58/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1329 - acc: 0.9494 - lr: 0.0010\n",
            "Epoch 59/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1329 - acc: 0.9473 - lr: 0.0010\n",
            "Epoch 60/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1298 - acc: 0.9483 - lr: 0.0010\n",
            "Epoch 61/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1258 - acc: 0.9536 - lr: 0.0010\n",
            "Epoch 62/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1420 - acc: 0.9473 - lr: 0.0010\n",
            "Epoch 63/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1236 - acc: 0.9599 - lr: 0.0010\n",
            "Epoch 64/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1039 - acc: 0.9641 - lr: 0.0010\n",
            "Epoch 65/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1005 - acc: 0.9652 - lr: 0.0010\n",
            "Epoch 66/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0841 - acc: 0.9757 - lr: 0.0010\n",
            "Epoch 67/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1105 - acc: 0.9568 - lr: 0.0010\n",
            "Epoch 68/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0943 - acc: 0.9694 - lr: 0.0010\n",
            "Epoch 69/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1079 - acc: 0.9599 - lr: 0.0010\n",
            "Epoch 70/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0844 - acc: 0.9736 - lr: 0.0010\n",
            "Epoch 71/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0940 - acc: 0.9684 - lr: 0.0010\n",
            "Epoch 72/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1118 - acc: 0.9557 - lr: 0.0010\n",
            "Epoch 73/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1470 - acc: 0.9451 - lr: 0.0010\n",
            "Epoch 74/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.1049 - acc: 0.9631 - lr: 0.0010\n",
            "Epoch 75/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0966 - acc: 0.9610 - lr: 0.0010\n",
            "Epoch 76/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0806 - acc: 0.9705 - lr: 0.0010\n",
            "Epoch 77/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0685 - acc: 0.9800 - lr: 0.0010\n",
            "Epoch 78/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0679 - acc: 0.9789 - lr: 0.0010\n",
            "Epoch 79/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0799 - acc: 0.9757 - lr: 0.0010\n",
            "Epoch 80/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0803 - acc: 0.9694 - lr: 0.0010\n",
            "Epoch 81/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.1030 - acc: 0.9631 - lr: 0.0010\n",
            "Epoch 82/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0814 - acc: 0.9736 - lr: 0.0010\n",
            "Epoch 83/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0713 - acc: 0.9821 - lr: 0.0010\n",
            "Epoch 84/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0527 - acc: 0.9842 - lr: 0.0010\n",
            "Epoch 85/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0742 - acc: 0.9726 - lr: 0.0010\n",
            "Epoch 86/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0882 - acc: 0.9641 - lr: 0.0010\n",
            "Epoch 87/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0750 - acc: 0.9736 - lr: 0.0010\n",
            "Epoch 88/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0723 - acc: 0.9747 - lr: 0.0010\n",
            "Epoch 89/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0844 - acc: 0.9736 - lr: 0.0010\n",
            "Epoch 90/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0650 - acc: 0.9778 - lr: 0.0010\n",
            "Epoch 91/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0727 - acc: 0.9747 - lr: 0.0010\n",
            "Epoch 92/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0868 - acc: 0.9684 - lr: 0.0010\n",
            "Epoch 93/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0687 - acc: 0.9778 - lr: 0.0010\n",
            "Epoch 94/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0716 - acc: 0.9757 - lr: 0.0010\n",
            "Epoch 95/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0592 - acc: 0.9831 - lr: 1.0000e-04\n",
            "Epoch 96/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0449 - acc: 0.9884 - lr: 1.0000e-04\n",
            "Epoch 97/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0494 - acc: 0.9842 - lr: 1.0000e-04\n",
            "Epoch 98/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0551 - acc: 0.9821 - lr: 1.0000e-04\n",
            "Epoch 99/100\n",
            "15/15 [==============================] - 0s 14ms/step - loss: 0.0573 - acc: 0.9863 - lr: 1.0000e-04\n",
            "Epoch 100/100\n",
            "15/15 [==============================] - 0s 13ms/step - loss: 0.0670 - acc: 0.9789 - lr: 1.0000e-04\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test accuracy for Subject 0\n",
        "\n",
        "test_loss_1, test_acc_1 = model.evaluate(X_spatial_test_1, y_test_subject_1)\n",
        "\n",
        "print(\"Test accuracy\", test_acc_1)\n",
        "print(\"Test loss\", test_loss_1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fHoU2mFcwhai",
        "outputId": "ef7e05b3-5a76-4b2b-ddde-68410f5b6482"
      },
      "execution_count": 268,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 1s 10ms/step - loss: 1.8978 - acc: 0.6000\n",
            "Test accuracy 0.6000000238418579\n",
            "Test loss 1.8977808952331543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test accuracy across all subjects\n",
        "\n",
        "test_loss_all, test_acc_all = model.evaluate(X_spatial_test, y_test)\n",
        "\n",
        "print(\"Test accuracy\", test_acc_all)\n",
        "print(\"Test loss\", test_loss_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "N1IQp6BHxfNM",
        "outputId": "cfc44de9-fa6d-46d3-aec0-92dd7b452845"
      },
      "execution_count": 269,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 7ms/step - loss: 3.9685 - acc: 0.3995\n",
            "Test accuracy 0.3995485305786133\n",
            "Test loss 3.968545436859131\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_1 = model.predict(X_spatial_test)"
      ],
      "metadata": {
        "id": "Ir2Jt2gxW8LE"
      },
      "execution_count": 272,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matrix_one = confusion_matrix(y_test.argmax(axis=1), y_pred_1.argmax(axis=1))"
      ],
      "metadata": {
        "id": "NjWfTZMOV0mI"
      },
      "execution_count": 273,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_disp_one = ConfusionMatrixDisplay(matrix_one, display_labels=[769, 770, 771, 772]) \n",
        "log_disp_one.plot()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "gGEfXLyXV9bs",
        "outputId": "8c09e592-8249-403d-ccf7-dd9d52904392"
      },
      "execution_count": 275,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f1c4287b310>"
            ]
          },
          "metadata": {},
          "execution_count": 275
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(9):\n",
        "  indeces = [ i for i in range(len(person_test)) if person_test[i]==j]\n",
        "  X_subject= X_test[indeces]\n",
        "  y_subject = y_test[indeces]\n",
        "  X_spatial_subject = spatial_prep(X_subject)\n",
        "  X_spatial_subject = np.swapaxes(X_spatial_subject, 1,3)\n",
        "  X_spatial_subject = np.swapaxes(X_spatial_subject, 2,3)\n",
        "\n",
        "  test_loss_subject, test_acc_subject = model.evaluate(X_spatial_subject, y_subject)\n",
        "  print(\"Statistics for subject \", j)\n",
        "  print(\"Test accuracy\", test_acc_subject)\n",
        "  print(\"Test loss\", test_loss_subject)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "K7dLduBdWJg5",
        "outputId": "b6d1a4ea-85cc-4bde-9060-8edef903bd2f"
      },
      "execution_count": 276,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8978 - acc: 0.6000\n",
            "Statistics for subject  0\n",
            "Test accuracy 0.6000000238418579\n",
            "Test loss 1.8977808952331543\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.4162 - acc: 0.3400\n",
            "Statistics for subject  1\n",
            "Test accuracy 0.3400000035762787\n",
            "Test loss 4.416205406188965\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0131 - acc: 0.5600\n",
            "Statistics for subject  2\n",
            "Test accuracy 0.5600000023841858\n",
            "Test loss 2.013089656829834\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.6129 - acc: 0.3800\n",
            "Statistics for subject  3\n",
            "Test accuracy 0.3799999952316284\n",
            "Test loss 3.612865924835205\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.9795 - acc: 0.4043\n",
            "Statistics for subject  4\n",
            "Test accuracy 0.40425533056259155\n",
            "Test loss 4.979465961456299\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.1326 - acc: 0.2653\n",
            "Statistics for subject  5\n",
            "Test accuracy 0.26530611515045166\n",
            "Test loss 6.132593154907227\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.2692 - acc: 0.3800\n",
            "Statistics for subject  6\n",
            "Test accuracy 0.3799999952316284\n",
            "Test loss 3.2691924571990967\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.1357 - acc: 0.3800\n",
            "Statistics for subject  7\n",
            "Test accuracy 0.3799999952316284\n",
            "Test loss 4.135655879974365\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.4531 - acc: 0.2766\n",
            "Statistics for subject  8\n",
            "Test accuracy 0.27659574151039124\n",
            "Test loss 5.453065872192383\n"
          ]
        }
      ]
    }
  ]
}